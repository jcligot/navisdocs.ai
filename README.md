

# **Rapport Technique : Mise en Œuvre d'un Système de Recherche Neurale de Nouvelle Génération avec ColBERT (Interaction Tardive) et Vespa, Intégré à Paperless-ngx AI et Ollama pour Modèles LLM sur CPU, via Docker Compose**

## **1\. Introduction : Le Paysage de la Recherche Neurale et de la Génération Augmentée par la Récupération (RAG) de Nouvelle Génération**

La recherche neurale, ou récupération d'informations neuronales, représente une transformation fondamentale dans le domaine de la recherche d'informations. Elle dépasse les approches traditionnelles basées sur les mots-clés et même les méthodes de recherche vectorielle de base. En exploitant des réseaux de neurones profonds (DNN), la recherche neurale interprète la signification contextuelle et les relations sémantiques inhérentes aux requêtes des utilisateurs et aux données. Cette compréhension avancée aboutit à des résultats plus précis, adaptables et pertinents, et prend en charge de manière cruciale les données multimodales (texte, images, audio et ensembles de données 3D complexes), ce qui est essentiel pour les applications basées sur l'IA et les expériences utilisateur de nouvelle génération.1  
Une distinction clé de la recherche neurale réside dans son utilisation des DNN tout au long du pipeline de recherche, de l'encodage et de l'indexation des données à l'encodage des requêtes et aux mécanismes sophistiqués de correspondance sémantique et de classement. Cette application holistique de l'apprentissage profond permet un apprentissage continu et un affinement des résultats, surpassant constamment les méthodes de recherche conventionnelles en termes de précision et d'adaptabilité.1 Le processus fondamental implique la transformation de l'entrée de l'utilisateur et des données sous-jacentes en plongements vectoriels denses – des représentations numériques qui capturent efficacement les relations sémantiques entre les concepts. Le DNN compare ensuite directement ces plongements, évaluant les relations apprises pour classer les résultats en fonction de leur alignement contextuel, dépassant ainsi les limites de la correspondance exacte des termes.1  
La Génération Augmentée par la Récupération (RAG) est une technique d'optimisation pour les grands modèles de langage (LLM) qui améliore considérablement la qualité des prédictions. Elle permet aux LLM de référencer une base de connaissances externe et faisant autorité, en dehors de leurs sources de données d'entraînement d'origine, pendant la phase d'inférence. Cette approche aborde directement les limites courantes des LLM, telles que les inexactitudes factuelles (souvent appelées "hallucinations"), les informations obsolètes et un manque général de connaissances spécifiques à un domaine.3  
Le processus RAG introduit un composant de récupération d'informations qui utilise la requête d'entrée de l'utilisateur pour extraire d'abord des informations de "contexte" pertinentes d'un magasin de données désigné, généralement une base de données vectorielle. Ces informations pertinentes récupérées sont ensuite combinées intelligemment avec la requête originale de l'utilisateur pour construire une requête plus riche et mieux informée. Le LLM utilise ensuite cette requête augmentée, tirant parti des nouvelles connaissances ainsi que de ses données d'entraînement, pour générer des réponses plus précises, pertinentes et fondées.3 Le RAG offre des avantages substantiels : il peut atteindre une qualité de prédiction supérieure même avec des LLM ayant moins de paramètres, permet des mises à jour dynamiques des connaissances simplement en actualisant les corpus de récupération, et fournit des citations vérifiables pour que les utilisateurs puissent facilement évaluer et faire confiance aux prédictions générées. Il est particulièrement efficace pour incorporer des données en temps réel, des données utilisateur personnelles ou des informations spécifiques au contexte qui n'étaient pas disponibles pour le LLM au moment de son entraînement.3

### **Présentation de l'Architecture Proposée : ColBERT, Vespa, Paperless-ngx AI, Ollama et Docker Compose**

Ce rapport détaille la construction d'un système complet et auto-hébergé de Génération Augmentée par la Récupération (RAG), spécifiquement conçu pour la compréhension et la récupération intelligente de documents.

* **Paperless-ngx AI** fonctionnera comme la couche principale d'ingestion et de gestion des documents. Il gérera la reconnaissance optique de caractères (OCR) pour les documents numérisés, effectue l'extraction initiale des métadonnées et maintiendra une archive structurée et consultable de documents.5  
* **Vespa** servira de plateforme de recherche IA haute performance et de base de données vectorielle. Son rôle est d'indexer efficacement les plongements de documents générés et d'exécuter des requêtes de recherche neuronale sophistiquées à grande échelle.7  
* **ColBERT (Interaction Tardive)** sera intégré aux capacités de recherche de Vespa. La force de ColBERT réside dans sa correspondance sémantique fine au niveau des jetons, ce qui améliorera considérablement la pertinence et la précision des documents récupérés.9  
* **Ollama** sera utilisé pour héberger un grand modèle de langage (LLM) spécifiquement optimisé pour les environnements CPU uniquement. Cela permettra une génération de texte locale, privée et rentable, en tirant parti du contexte récupéré par Vespa.11  
* L'ensemble du système sera déployé et géré à l'aide de **Docker Compose**. Cet outil d'orchestration assurera une communication inter-services transparente, simplifiera le processus de configuration et facilitera la gestion continue des composants intégrés.13

L'intégration de la recherche neuronale et de la RAG pour la gestion de documents est essentielle. Alors que Paperless-ngx excelle dans l'organisation de base des documents, son système de recherche natif, basé sur le texte intégral et la correspondance de métadonnées simples, ne peut pas saisir les nuances sémantiques requises pour des requêtes complexes. La recherche neuronale, en particulier avec ColBERT, apporte cette profondeur sémantique, permettant une compréhension contextuelle qui va au-delà des correspondances exactes de mots. Le rôle de la RAG est alors de donner un but à cette récupération sémantique : fournir des réponses précises et vérifiables via un LLM. Cette combinaison transforme une archive statique en une base de connaissances dynamique et intelligente, améliorant considérablement la productivité et réduisant les efforts manuels de récupération d'informations. Le système agit comme un assistant proactif, capable de comprendre l'intention de l'utilisateur et le contexte de sa demande.2  
La contrainte de fonctionner sans GPU pour le LLM est un facteur déterminant pour la conception de l'ensemble du système. Elle a des implications profondes sur le choix des modèles (nécessitant des LLM plus petits et fortement quantifiés), sur la vitesse d'inférence attendue du LLM (plus lente qu'avec un GPU), et sur la nécessité d'une gestion efficace des ressources pour les plongements ColBERT au sein de Vespa. Cette exigence n'est pas un obstacle mineur, mais un moteur de conception principal, qui oriente l'attention vers des composants d'IA hautement optimisés et économes en ressources. Chaque décision, de la quantification du LLM aux stratégies d'indexation de Vespa pour ColBERT, doit donner la priorité à la minimisation de la charge de calcul afin de garantir une latence et une convivialité acceptables. Cette approche met en lumière une tendance croissante dans l'IA : la recherche d'une IA accessible, locale et respectueuse de la vie privée. Le scénario "sans GPU" repousse les limites de ce qui est réalisable sur du matériel grand public, démocratisant ainsi l'IA avancée, mais exigeant une ingénierie méticuleuse. Le succès de cette solution auto-hébergée dépend directement de cette optimisation multicouche.

## **2\. Plongée Profonde dans les Technologies Clés**

### **ColBERT : Interaction Tardive Contextualisée pour une Récupération Fine**

ColBERT (Contextualized Late Interaction over BERT) se distingue des approches bi-encodeurs standard en générant une *représentation matricielle (multivectorielle)* du texte d'entrée, plutôt qu'un seul vecteur de dimension fixe pour l'ensemble de la requête ou du document. Cela signifie qu'il produit un plongement vectoriel distinct pour chaque jeton (ou sous-mot) dans le texte d'entrée. Cette approche de "sac de vecteurs" permet à ColBERT de capturer des sémantiques d'entrée plus nuancées et des relations sémantiques plus fines par rapport aux modèles qui compressent une entrée entière en un seul vecteur.9  
Le processus implique la tokenisation de la requête et du document (par exemple, à l'aide du tokenizer WordPiece de BERT). Chaque jeton, ainsi que son contexte environnant, est ensuite traité via une architecture d'encodeur de transformateur modifiée (telle que XLM-RoBERTa, comme on le voit dans Jina-ColBERT-v2). Cela produit un plongement contextualisé pour chaque jeton. Pour maintenir la convivialité pratique et la compatibilité avec l'infrastructure existante de similarité vectorielle, la dimensionnalité de sortie de ces plongements de jetons est généralement contrainte (par exemple, réduite de 128 à 64 dimensions dans ColBERTv2, avec un compromis de performance minimal). Malgré cela, le stockage de plusieurs vecteurs par document nécessite intrinsèquement plus d'espace qu'une représentation à vecteur unique.9  
Le trait distinctif de ColBERT est son mécanisme d'"interaction tardive". Contrairement aux cross-encodeurs, qui concatènent une requête et un document en une seule entrée pour un traitement conjoint ("interaction précoce"), ColBERT encode les requêtes et les documents *indépendamment*. L'interaction cruciale, ou comparaison, se produit *plus tard*, pendant la phase de notation.9 Le score de pertinence entre une requête et un document est calculé en fonction des similarités entre leurs plongements de jetons respectifs. Plus précisément, pour chaque jeton de la requête, sa similarité (généralement en utilisant le produit scalaire) est calculée par rapport à  
*tous* les plongements de jetons du document. La similarité maximale (MaxSim) trouvée pour chaque jeton de requête individuel est ensuite identifiée. Le score de pertinence global pour le document est la *somme de ces valeurs MaxSim* sur tous les jetons de la requête.15 Cette méthode de score par interaction tardive approche efficacement l'attention conjointe requête-document observée dans les cross-encodeurs plus coûteux en calcul, tout en maintenant une efficacité d'inférence plus proche de celle des modèles de récupération dense traditionnels. Cet équilibre rend ColBERT adapté aux applications à grande échelle nécessitant une grande précision sans coûts de calcul prohibitifs.9  
Les plongements multivectoriels générés par ColBERT sont parfaitement compatibles avec le support multivectoriel de Vespa, ce qui constitue un levier de performance essentiel. L'intégration native permet d'indexer les plongements au niveau des jetons directement dans Vespa. Cela signifie que le calcul intensif de MaxSim, qui est au cœur de la notation de pertinence fine de ColBERT, peut être effectué efficacement au sein du moteur de classement hautement optimisé et distribué de Vespa. Cette approche minimise les frais généraux de transfert de données et tire parti des capacités de traitement parallèle de Vespa, rendant la précision de ColBERT évolutive et performante dans un environnement de production. L'efficacité des modèles de récupération neuronale avancés comme ColBERT dans un système à grande échelle dépend fortement de la capacité du moteur de recherche sous-jacent à prendre en charge nativement et à traiter efficacement leurs structures d'intégration uniques et leurs mécanismes d'interaction. Cette intégration est un facteur clé pour obtenir une RAG de haute qualité.  
Les avantages de ColBERT et ses cas d'utilisation, notamment le reranking dans les systèmes RAG, sont multiples. Sa correspondance fine, grâce à la comparaison jeton par jeton et à l'agrégation MaxSim, permet à ColBERT de se concentrer sur les parties les plus pertinentes d'un document, ce qui conduit à une précision et une robustesse supérieures, en particulier pour les requêtes nuancées ou hors domaine.9 Un avantage significatif est la capacité de précalculer et d'indexer les plongements de documents, ce qui rend la récupération beaucoup plus rapide que les cross-encodeurs. Par conséquent, ColBERT est souvent recommandé pour le reranking d'un ensemble plus petit de documents candidats (par exemple, 100 à 500\) initialement récupérés par un récupérateur dense plus simple et plus rapide, plaçant ainsi les résultats les plus pertinents en tête.10 La conscience du contexte, inhérente à la conception de ColBERT, lui permet de saisir la signification nuancée des mots, même lorsqu'ils apparaissent dans des contextes différents.17 Dans les systèmes RAG, la précision et l'efficacité de ColBERT le rendent exceptionnellement efficace en tant que reranker, car une récupération précise et contextuellement riche des documents a un impact direct sur la qualité et l'exactitude factuelle des réponses générées par le LLM.17 Bien que ColBERTv2 soit monolingue, des extensions multilingues comme ColBERT-XM ont été développées pour répondre aux besoins de récupération multilingue.9  
Le tableau suivant compare les architectures de modèles de récupération neuronale :  
**Tableau 1 : Comparaison des Architectures de Modèles de Récupération Neurale**

| Caractéristique / Type de Modèle | Bi-encodeur (ex: Sentence-BERT) | Cross-encodeur (ex: BERT-base) | ColBERT (Interaction Tardive Contextualisée) |
| :---- | :---- | :---- | :---- |
| **Mécanisme d'Interaction** | Encodage indépendant, similarité précoce | Encodage conjoint, interaction précoce | Encodage indépendant, interaction tardive |
| **Encodage** | Requête et document encodés séparément en un seul vecteur chacun | Requête et document concaténés et encodés ensemble en une seule séquence | Requête et document encodés séparément en *plusieurs* vecteurs (un par jeton) |
| **Calcul de Similarité** | Produit scalaire ou distance cosinus entre les vecteurs uniques | Attention conjointe au sein du modèle, score de pertinence final | MaxSim (similarité maximale par jeton de requête, puis somme) |
| **Efficacité d'Inférence** | Très rapide (plongements de document précalculés) | Très lent (nécessite un nouveau calcul pour chaque paire requête-document) | Rapide (plongements de document précalculés, interaction tardive efficace) |
| **Précision** | Moins précise pour les requêtes fines (perte de contexte) | Très précise (capture les interactions fines) | Très précise (approche les cross-encodeurs avec une meilleure efficacité) |
| **Coût de Calcul** | Faible | Élevé | Modéré (compromis équilibré) |
| **Cas d'Usage Typique** | Récupération initiale à grande échelle, filtrage | Re-classement de précision sur un petit ensemble de candidats | Re-classement dans les pipelines RAG, recherche sémantique fine |

### **Vespa : La Plateforme de Recherche IA et de Base de Données Vectorielle**

Vespa est une plateforme avancée conçue pour les applications nécessitant un calcul à faible latence sur des ensembles de données massifs. Elle peut gérer des systèmes allant jusqu'à des centaines de nœuds, gérant des dizaines de milliards de documents et des milliers de requêtes par seconde, avec des temps de réponse typiques de l'ordre de quelques dizaines de millisecondes. Son architecture garantit des temps de réponse constants quel que soit le volume de données en exécutant les requêtes en parallèle sur de nombreux shards de données et cœurs.7  
Une capacité fondamentale de Vespa est son support natif des tenseurs, qui facilite le classement et la prise de décision complexes. Vespa intègre l'inférence de modèles appris par machine distribués pour la notation de pertinence, ce qui en fait un choix idéal pour les applications d'IA en temps réel telles que la RAG, les moteurs de recommandation et la recherche intelligente à l'échelle de l'entreprise.7 Vespa est conçu pour les changements de données continus, supportant des volumes d'écriture élevés (des milliers à des dizaines de milliers par nœud et par seconde) simultanément avec le service de requêtes. Il maintient automatiquement une distribution équilibrée des données et une redondance configurable, même lorsque des nœuds sont ajoutés, supprimés ou perdus de manière inattendue.7  
Vespa offre de solides capacités d'indexation multivectorielle, permettant aux applications de stocker et de gérer plusieurs vecteurs par document. Cette fonctionnalité est cruciale pour l'intégration de ColBERT, qui produit des plongements au niveau des jetons, car elle permet de récupérer des documents en fonction du vecteur le plus proche dans un document par rapport au vecteur de la requête.7 Il fournit une fonctionnalité de recherche de voisins les plus proches approximatifs (ANN) via un algorithme de graphe Hierarchical Navigable Small World (HNSW hautement optimisé). Cette méthode est nettement plus rentable pour les grands ensembles de données que la recherche exacte (force brute). L'implémentation HNSW de Vespa prend en charge le filtrage, l'indexation multivectorielle et les opérations CRUD en temps réel sur les vecteurs, garantissant des index dynamiques et à jour.7 L'opérateur de requête  
nearestNeighbor est central pour la recherche vectorielle dans Vespa et peut être combiné de manière transparente avec des filtres structurés et des opérateurs de recherche de texte traditionnels à l'aide du langage de requête Vespa (YQL). Cela permet la création de solutions de récupération hybrides puissantes qui exploitent à la fois les signaux sémantiques et lexicaux.7 Vespa prend en charge une variété de métriques de distance (par exemple,  
euclidean, angular, dotproduct, prenormalized-angular, hamming, geodegrees), qui sont configurées au niveau du champ tenseur et utilisées à la fois pour la construction de l'index et le calcul de la distance au moment de la requête.7  
Le classement dans Vespa est configuré via des expressions de classement déclaratives définies dans des "profils de classement" dans les schémas de documents. Ces expressions peuvent aller d'opérations mathématiques de base utilisant des fonctionnalités de classement intégrées à des expressions tensorielles très complexes et des modèles appris par machine intégrés (prenant en charge des formats tels que ONNX, XGBoost et LightGBM).7 La capacité de classement par phases de Vespa est une optimisation puissante. Elle permet plusieurs étapes de classement : une expression  
first-phase peu coûteuse est appliquée à tous les documents récupérés, suivie d'une second-phase plus intensive en calcul (et éventuellement une global-phase) exécutée uniquement sur les documents les mieux classés de la phase précédente. Cette stratégie dirige efficacement plus de calcul vers les documents candidats les plus prometteurs, optimisant considérablement les performances globales des requêtes.7 Pour implémenter l'interaction tardive de ColBERT, la capacité de Vespa à définir des fonctions de classement personnalisées à l'aide de tenseurs est essentielle. La fonction de classement  
closeness() peut être utilisée avec des champs multivectoriels pour trouver le vecteur le plus proche, ce qui est un composant clé pour MaxSim. Vespa prend également en charge unpack\_bits pour gérer les vecteurs binaires compressés, ce qui peut être particulièrement utile pour le stockage économe en mémoire des vecteurs de jetons de ColBERT.7

### **Paperless-ngx AI : Gestion Intelligente des Documents**

Paperless-ngx est un système de gestion de documents open source conçu pour transformer les documents physiques et numériques en une archive consultable et organisée. Il effectue la reconnaissance optique de caractères (OCR) sur les documents sans texte intégré à l'aide du moteur open source Tesseract, qui prend en charge plus de 100 langues. Les documents sont stockés au format PDF/A pour l'archivage à long terme, aux côtés de leurs originaux non altérés.6  
Une caractéristique intelligente essentielle est son utilisation de l'apprentissage automatique pour attribuer automatiquement des métadonnées telles que des étiquettes, des correspondants, des types de documents et des chemins de stockage. Les utilisateurs peuvent configurer divers algorithmes de correspondance (par exemple, "Any", "All", "Exact", "Regular expression", "Fuzzy match", ou "Auto") ou utiliser un mode manuel avec assistance IA pour la révision.5 Les fonctionnalités clés incluent une puissante recherche en texte intégral avec auto-complétion et surlignage, une fonction "Plus comme ça" pour trouver des documents similaires, de solides capacités de traitement des e-mails, un système de permissions multi-utilisateurs robuste et des flux de travail personnalisables pour automatiser la gestion des documents.6  
L'extension "Paperless-AI" augmente considérablement les capacités de Paperless-ngx en intégrant des fonctions d'IA avancées pour l'analyse automatisée des documents et même une fonction de chat interactive permettant de poser des questions directement sur les documents.5 Un avantage crucial pour les utilisateurs soucieux de la confidentialité et les scénarios d'auto-hébergement est la flexibilité de choisir les backends d'IA. Cela inclut la possibilité d'utiliser des services basés sur le cloud (comme OpenAI ou Azure) ou, de manière cruciale, de tirer parti des  
*modèles locaux via Ollama*.5 La politique de confidentialité du projet Paperless-AI stipule explicitement qu'il agit comme une interface, envoyant le contenu des documents crypté uniquement au serveur Paperless-AI configuré, sans aucune donnée transmise aux développeurs ou à des tiers, renforçant ainsi le contrôle local des données.5  
Paperless-ngx expose une API REST complète, offrant diverses méthodes d'authentification (Basic, Session, Token et Remote User), permettant une interaction programmatique avec le système.27 L'API prend en charge les requêtes de recherche en texte intégral (  
/api/documents/?query=...) et les recherches "plus comme ça" (/api/documents/?more\_like\_id=...), renvoyant des résultats avec un score de pertinence, des highlights des termes correspondants et un rank.27 Au-delà de la recherche, l'API fournit des méthodes pour gérer les documents et leurs métadonnées, y compris la définition des correspondants, des types de documents, des chemins de stockage, l'ajout/la suppression d'étiquettes, la suppression, le retraitement, la fusion, le fractionnement, la rotation des pages et la modification des champs personnalisés.27 Les documents peuvent être téléchargés via l'interface utilisateur, et le système stocke systématiquement les documents originaux tout en créant également des versions PDF/A archivables.26 Pour la gestion des données en vrac, un outil  
document\_exporter est disponible, capable d'exporter tous les documents, vignettes, métadonnées et contenus de base de données vers un dossier spécifié. Cette fonctionnalité est essentielle pour les sauvegardes incrémentielles ou la migration de documents vers d'autres systèmes de gestion de documents (DMS).29  
L'intégration des métadonnées de Paperless-ngx et de la recherche sémantique de ColBERT crée une approche de récupération hybride puissante. Paperless-ngx excelle à fournir des attributs structurés pour les documents (tags, correspondants, types de documents). ColBERT, quant à lui, offre une compréhension sémantique approfondie. Lorsque ces capacités sont combinées, les métadonnées de Paperless-ngx peuvent servir de filtres très efficaces dans les requêtes de Vespa, réduisant rapidement l'espace de recherche à un sous-ensemble pertinent de documents. Après ce filtrage initial par métadonnées structurées, la recherche sémantique de ColBERT peut effectuer son re-classement fin au niveau des jetons sur cet ensemble de candidats plus petit et plus ciblé. Cela crée un pipeline de récupération hybride multi-étapes qui tire parti de la précision des données structurées pour la récupération initiale et de la flexibilité de la compréhension sémantique pour un classement nuancé. Il en résulte une expérience de recherche plus robuste et conviviale. Les utilisateurs peuvent exploiter simultanément les attributs explicites des documents (par exemple, "afficher les factures du fournisseur X") et le contenu sémantique implicite (par exemple, "trouver des documents sur la restructuration financière"), ce qui conduit à des réponses RAG plus précises et plus efficaces. C'est une approche qui combine le meilleur des deux mondes pour la récupération d'informations.

### **Ollama : Inférence LLM Locale pour les Environnements CPU Uniquement**

Ollama (Open LLM Application Manager) est une plateforme open source spécifiquement conçue pour déployer et interagir avec des grands modèles de langage (LLM) open source localement, prenant en charge l'inférence CPU et GPU.11 Pour l'inférence CPU uniquement, Ollama peut offrir des performances acceptables pour certains cas d'utilisation, tels que les opérations par lots importantes ou l'accès programmatique où des réponses immédiates et en temps réel ne sont pas strictement requises. Cependant, elle sera nettement plus lente que les solutions basées sur GPU, avec des retards potentiels de 30 à 60 secondes pour les opérations par lots, contre 2 à 10 secondes sur GPU. Pour les questions interactives complexes, il peut y avoir un délai de 5 à 10 secondes avant la réponse, et la génération de texte peut sembler lente, rappelant les vitesses des modems plus anciens.12 Les exigences matérielles pour l'inférence CPU uniquement sont substantielles, nécessitant généralement un CPU relativement puissant (par exemple, un équivalent Intel i7) et des quantités importantes de RAM. Les modèles plus petits (par exemple, 7 milliards de paramètres) peuvent nécessiter 16 à 32 Go de RAM, tandis qu'un modèle de 14 milliards de paramètres pourrait nécessiter environ 20 Go de RAM.12 Les performances attendues sur une configuration CPU uniquement avec des modèles quantifiés comme Llama 2 ou Mistral-7B se situent généralement dans la plage de 5 à 10 jetons par seconde.33  
La quantification est une stratégie essentielle et fréquemment appliquée pour rendre les modèles d'apprentissage automatique volumineux et complexes, en particulier les LLM, suffisamment légers pour fonctionner dans des environnements contraints en ressources comme les machines locales sans GPU dédiés. Elle implique la réduction de la précision numérique des paramètres (poids) du modèle, généralement de la virgule flottante 32 bits à des représentations inférieures comme les entiers 8 bits ou 4 bits. Cela réduit considérablement l'empreinte mémoire et accélère la vitesse d'inférence.30 Ollama prend en charge de manière transparente un large éventail de modèles quantifiés, souvent au format GGUF, qui est spécifiquement optimisé pour l'inférence sur machine locale.30  
Les LLM optimisés pour le CPU recommandés (souvent disponibles en versions quantifiées 4 bits ou 8 bits pour l'efficacité) incluent :

* **Mistral 7B (versions quantifiées) :** Loué pour son excellent rapport performance/taille et ses capacités de raisonnement décentes, ce qui en fait un candidat solide pour les configurations CPU uniquement.12  
* **Llama 2 (7B ou 13B, quantifié) :** Un modèle largement pris en charge et capable pour les tâches générales de traitement du langage naturel. Les versions quantifiées sont essentielles pour s'adapter aux limites de RAM CPU typiques et optimiser les performances.30  
* **Gemma 7B (quantifié) :** En tant que grand frère de la famille Gemma, il offre des capacités améliorées et partage des composants architecturaux avec les modèles Gemini plus grands de Google, permettant des performances élevées sur du matériel grand public. Les versions quantifiées (par exemple, Q5\_K\_M avec une taille de fichier de 6,14 Go) peuvent tenir confortablement dans 8 Go de RAM système pour des performances optimales.30  
* **Phi-3 Mini (3.8B, quantifié) :** Le modèle léger et de pointe de Microsoft, distingué par une efficacité exceptionnelle et une forte concentration sur des propriétés de haute qualité et de raisonnement dense. Il est remarquablement économe en mémoire (par exemple, la quantification Q8\_0 est un fichier de 4,06 Go, nécessitant environ 7,48 Go de mémoire), ce qui le rend adapté aux environnements contraints en mémoire/calcul et aux scénarios à latence limitée.35  
* **Hermes 3 Llama 3.2 3B/8B (quantifié) :** Noté pour son obéissance et ses solides performances, en particulier lorsqu'il utilise des balises d'appel de fonction.33  
* **Modèles TinyLLaMA :** Impressionnants malgré leur très petite empreinte, ce qui les rend très accessibles pour les configurations minimales.32

Pour les systèmes CPU uniquement, une quantification agressive (par exemple, 4 bits) est fortement recommandée, car elle donne souvent de meilleures performances globales que les modèles légèrement plus grands et moins agressivement quantifiés.32  
La quantification des modèles par Ollama constitue un pont vers une IA locale omniprésente. La capacité d'Ollama à exécuter efficacement des LLM quantifiés sur du matériel CPU uniquement est un facteur fondamental pour répondre à l'exigence d'un système "auto-hébergé" et "sans GPU". Cette capacité rend l'IA générative puissante accessible sans les coûts prohibitifs ou la dépendance au cloud des solutions accélérées par GPU. La quantification "démocratise" l'accès aux LLM avancés, déplaçant l'accent de la puissance de calcul brute vers la compression intelligente des modèles et des piles logicielles efficaces. Cela transforme les LLM de technologies exclusives aux GPU en outils viables pour les déploiements locaux, personnels ou à petite échelle en entreprise. Cela signifie que le système peut être entièrement privé et maîtrisé en termes de coûts. Cependant, cela s'accompagne d'un compromis inhérent de vitesses d'inférence plus lentes, nécessitant une sélection rigoureuse des LLM et une gestion vigilante de la RAM. Cette stratégie s'aligne sur la demande croissante de solutions d'IA soucieuses de la confidentialité et économes en ressources.  
Par défaut, Ollama charge un modèle spécifié en mémoire sur demande. Cependant, si les requêtes API ultérieures (par exemple, provenant de différentes applications ou avec des paramètres variables comme context\_size) introduisent des incompatibilités de configuration, Ollama peut décharger puis recharger le modèle. Cette permutation fréquente augmente considérablement la latence et l'utilisation des ressources.36 Pour assurer un chargement persistant du modèle et minimiser ces retards, le paramètre  
duration: \-1 peut être défini lors du chargement d'un modèle via le point de terminaison /api/load d'Ollama. Cette configuration indique à Ollama de maintenir le modèle chargé en mémoire indéfiniment, quelle que soit l'activité ultérieure ou les requêtes de point de terminaison.36 De plus, la standardisation de la  
context\_size sur toutes les applications ou processus interagissant avec la même instance Ollama est cruciale. Des paramètres de contexte cohérents empêchent le rechargement du modèle pour s'adapter à différentes configurations, contribuant ainsi à des performances stables et efficaces.36

## **3\. Architecture Détaillée et Mise en Œuvre avec Docker Compose**

La mise en place de ce système de recherche neurale et RAG auto-hébergé repose sur une architecture modulaire et une orchestration efficace via Docker Compose.

### **3.1. Structure de l'Application Vespa**

Vespa est le cœur de la recherche et du classement. Pour intégrer ColBERT, une configuration spécifique est nécessaire.

* **Schéma de Document pour les Plongements ColBERT :** Un schéma de document Vespa doit être défini pour stocker les plongements multivectoriels de ColBERT. Chaque document dans Paperless-ngx (ou ses fragments) sera représenté dans Vespa par un champ tenseur multivectoriel. Par exemple, un champ colbert\_embeddings de type tensor\<float\>(token{}, x{}) peut être utilisé, où token{} représente les différents jetons et x{} la dimension du plongement de chaque jeton (par exemple, 64 ou 128). L'indexation HNSW sera activée sur ce champ pour permettre une recherche ANN efficace.7  
* **Profils de Classement pour l'Interaction Tardive :** Pour implémenter le mécanisme d'interaction tardive de ColBERT (MaxSim), des profils de classement personnalisés doivent être définis dans Vespa. Cela implique l'utilisation de fonctions tensorielles pour calculer la similarité entre les plongements de jetons de la requête et du document. La fonction closeness() de Vespa peut être utilisée pour trouver le vecteur le plus proche dans un champ multivectoriel. Des expressions de classement complexes peuvent être construites pour agréger ces similarités au niveau des jetons, reflétant le score MaxSim de ColBERT. L'utilisation de phases de classement (par exemple, first-phase pour une récupération rapide et second-phase pour un reranking précis avec ColBERT) est essentielle pour optimiser les performances.7  
* **Intégration de l'Embedder ColBERT :** Vespa peut intégrer des embedders directement. Il est possible d'utiliser l'embedder natif de Vespa pour ColBERT ou d'intégrer un modèle ColBERT pré-entraîné (par exemple, colbert-ir/colbertv2.0 via ONNX) pour générer les plongements de jetons au moment de l'ingestion des documents et des requêtes.8  
* **Gestion des Données Multimodales :** Bien que l'accent soit mis sur le texte, Vespa prend en charge l'indexation multimodale. Si Paperless-ngx AI venait à extraire des informations visuelles (par exemple, via ColiPali), Vespa pourrait indexer ces plongements d'images à côté des plongements de texte, permettant une recherche encore plus riche.16

### **3.2. Configuration de Paperless-ngx AI pour l'Extraction et l'Exportation**

Paperless-ngx AI gérera l'ingestion des documents et l'extraction initiale des métadonnées, agissant comme la source de données pour Vespa.

* **Ingestion et OCR :** Paperless-ngx surveille un dossier de consommation ou reçoit des documents via son interface web ou API. Il effectue l'OCR sur les documents sans texte intégré, convertissant les images en texte consultable. Les documents sont stockés au format PDF/A.6  
* **Extraction Automatisée de Métadonnées :** Paperless-ngx utilise l'apprentissage automatique pour attribuer automatiquement des tags, des correspondants et des types de documents. Ces métadonnées structurées sont cruciales pour le filtrage dans Vespa.5  
* **Accès aux Données pour l'Indexation Vespa :** Pour que Vespa puisse indexer les documents et leurs métadonnées, plusieurs approches sont possibles :  
  * **API Paperless-ngx :** L'API REST de Paperless-ngx permet de récupérer le contenu textuel des documents et leurs métadonnées (tags, correspondants, etc.).27 Un script externe pourrait interroger cette API, générer les plongements ColBERT et les envoyer à Vespa.  
  * **Exportateur de Documents :** L'outil document\_exporter de Paperless-ngx peut exporter tous les documents, les vignettes, les métadonnées et le contenu de la base de données vers un dossier spécifié. Cela est idéal pour les chargements initiaux en masse ou les mises à jour par lots, permettant à un processus externe de lire ces données, de générer des plongements et de les alimenter à Vespa.29  
  * **Accès Direct aux Volumes :** Si les conteneurs Docker sont configurés avec des volumes partagés, un service dédié pourrait accéder directement aux fichiers de documents et à la base de données de Paperless-ngx (par exemple, SQLite ou PostgreSQL) pour extraire les informations nécessaires. Cependant, cela nécessite une gestion attentive des dépendances et de la cohérence des données.29

### **3.3. Configuration d'Ollama pour l'Inférence LLM sur CPU**

Ollama sera le moteur du LLM, fournissant des capacités de génération de texte basées sur le contexte récupéré.

* **Sélection du Modèle LLM sans GPU :** Pour une inférence sur CPU uniquement, il est impératif de choisir des modèles LLM petits et fortement quantifiés. Les modèles comme Mistral 7B (quantifié), Llama 2 (7B ou 13B, quantifié), Gemma 7B (quantifié) ou Phi-3 Mini sont des candidats appropriés. La quantification (par exemple, 4 bits) est essentielle pour réduire l'empreinte mémoire et améliorer la vitesse d'inférence sur CPU.30  
* **Persistance du Modèle :** Pour éviter les rechargements fréquents du modèle qui augmentent la latence, il est recommandé de configurer Ollama pour maintenir le modèle chargé en mémoire en utilisant le paramètre duration: \-1 via le point de terminaison /api/load. De plus, la standardisation de la context\_size sur toutes les applications interagissant avec Ollama est cruciale.36  
* **Exposition de l'API Ollama :** Ollama expose une API (généralement sur le port 11434\) qui sera utilisée par l'application RAG pour envoyer des requêtes et recevoir des réponses du LLM.13

### **3.4. Orchestration avec Docker Compose**

Docker Compose simplifiera le déploiement et la gestion des services interconnectés.

* **Réseau Docker Partagé :** Un réseau Docker Compose unique sera créé par défaut, permettant à tous les services (Vespa, Paperless-ngx, Ollama et tout service intermédiaire) de communiquer entre eux en utilisant leurs noms de service comme noms d'hôte.39  
* **Structure du Fichier docker-compose.yml :** Le fichier docker-compose.yml définira les services suivants :  
  * **vespa :** Utilise l'image vespaengine/vespa. Les volumes persistants seront configurés pour les données de Vespa. Les ports nécessaires (par exemple, 8080 pour les requêtes/alimentation, 19071 pour le déploiement) seront mappés vers l'hôte.41  
  * **paperless-ngx :** Utilise l'image Paperless-ngx. Des volumes seront montés pour les documents (media), les données (data) et la base de données (pgdata ou dbdata).29 Les variables d'environnement pour la configuration de Paperless-ngx seront définies.37  
  * **ollama :** Utilise l'image ollama/ollama. Un volume persistant sera configuré pour stocker les modèles LLM téléchargés (/root/.ollama). Le port 11434 sera mappé vers l'hôte pour l'accès à l'API.13  
  * **Service Intermédiaire (Python/Node.js) :** Un service personnalisé sera nécessaire pour orchestrer le flux RAG :  
    * Récupérer le contenu et les métadonnées de Paperless-ngx (via API ou export).  
    * Générer les plongements ColBERT pour les documents et les envoyer à Vespa pour l'indexation.  
    * Recevoir les requêtes de l'utilisateur.  
    * Générer les plongements ColBERT pour la requête de l'utilisateur.  
    * Interroger Vespa pour la récupération de documents pertinents (en utilisant les métadonnées de Paperless-ngx comme filtres et ColBERT pour le classement sémantique).  
    * Envoyer le contexte récupéré et la requête de l'utilisateur à Ollama pour la génération de réponses.  
    * Retourner la réponse du LLM à l'utilisateur.  
* **Volumes Persistants :** L'utilisation de volumes nommés Docker est cruciale pour garantir la persistance des données (documents Paperless-ngx, index Vespa, modèles Ollama) même si les conteneurs sont arrêtés ou recréés.13

### **3.5. Flux de Données et d'Interaction**

1. **Ingestion de Documents :** Les documents sont ajoutés à Paperless-ngx (via numérisation, e-mail, glisser-déposer).23 Paperless-ngx effectue l'OCR et l'extraction initiale des métadonnées.6  
2. **Indexation des Documents (Vespa) :** Un service intermédiaire (par exemple, un script Python) surveille les nouveaux documents dans Paperless-ngx (via API ou exportateur). Pour chaque document, il :  
   * Extrait le texte et les métadonnées.  
   * Génère les plongements multivectoriels ColBERT pour le contenu du document.  
   * Envoie le document (texte, métadonnées, plongements ColBERT) à Vespa pour l'indexation. Vespa stocke les plongements et construit son index HNSW.7  
3. **Requête Utilisateur :** L'utilisateur soumet une requête au service intermédiaire.  
4. **Récupération Hybride (Vespa) :** Le service intermédiaire :  
   * Génère les plongements multivectoriels ColBERT pour la requête de l'utilisateur.  
   * Construit une requête Vespa qui combine les filtres basés sur les métadonnées de Paperless-ngx (par exemple, type de document, correspondant) avec la recherche de voisins les plus proches sur les plongements ColBERT.  
   * Vespa exécute cette requête, utilisant son classement par phases pour récupérer un ensemble de documents pertinents, avec ColBERT assurant un classement fin.7  
5. **Génération Augmentée (Ollama) :** Les extraits de documents les plus pertinents récupérés par Vespa sont envoyés, avec la requête originale de l'utilisateur, à l'API Ollama. Le LLM hébergé par Ollama génère une réponse basée sur ce contexte fourni, garantissant une réponse fondée et pertinente.3  
6. **Réponse à l'Utilisateur :** Le service intermédiaire renvoie la réponse générée par le LLM à l'utilisateur.

## **4\. Conclusions et Recommandations**

La mise en œuvre d'un système de recherche neurale de nouvelle génération avec ColBERT et Vespa, intégré à Paperless-ngx AI et Ollama sur une architecture sans GPU, est une entreprise ambitieuse mais réalisable qui offre des avantages significatifs. Le système proposé transforme la gestion de documents en une expérience de connaissance interactive et intelligente.  
La synergie entre la gestion structurée des documents de Paperless-ngx et la compréhension sémantique fine de ColBERT, orchestrée par les capacités de recherche et de classement de Vespa, permet une récupération d'informations d'une précision inégalée. Les métadonnées de Paperless-ngx agissent comme des filtres puissants pour affiner rapidement l'ensemble des documents pertinents, tandis que ColBERT excelle dans le reclassement de cet ensemble avec une compréhension contextuelle profonde. Cette approche hybride combine le meilleur de la recherche basée sur les attributs et de la recherche sémantique.  
L'intégration d'Ollama sur un environnement CPU uniquement, bien que présentant des compromis en termes de vitesse d'inférence, est rendue viable par la sélection rigoureuse de modèles LLM petits et fortement quantifiés. Cette stratégie permet de maintenir un système entièrement auto-hébergé, garantissant une confidentialité totale des données et éliminant les coûts d'API récurrents. Docker Compose simplifie considérablement le déploiement et la gestion de cette architecture complexe, en assurant une communication fluide entre les services et une persistance des données.  
**Recommandations pour la Mise en Œuvre :**

1. **Prioriser la Quantification des LLM :** Étant donné la contrainte "sans GPU", il est impératif de se concentrer sur l'utilisation de modèles LLM fortement quantifiés (par exemple, 4 bits) avec Ollama. Des tests de performance rigoureux avec différents modèles (Mistral 7B, Llama 2 7B/13B, Gemma 7B, Phi-3 Mini) sont recommandés pour trouver le meilleur équilibre entre qualité de réponse et vitesse d'inférence sur le matériel CPU disponible.  
2. **Optimiser l'Indexation ColBERT dans Vespa :** Tirer pleinement parti du support multivectoriel natif de Vespa pour les plongements ColBERT. Configurer les profils de classement de Vespa pour exécuter efficacement le calcul MaxSim, en utilisant des phases de classement pour optimiser les performances de récupération et de re-classement.  
3. **Mettre en Place un Service d'Orchestration Robuste :** Développer un service intermédiaire (par exemple, en Python) qui gère l'extraction des données de Paperless-ngx, la génération des plongements ColBERT, l'alimentation de Vespa, l'interrogation de Vespa et l'interaction avec Ollama. Ce service doit être conçu pour gérer les erreurs et assurer la cohérence des données.  
4. **Gérer la Persistance des Modèles Ollama :** Configurer Ollama pour maintenir les modèles LLM chargés en mémoire en utilisant le paramètre duration: \-1 et en standardisant la context\_size pour toutes les interactions. Cela minimisera les délais de rechargement et améliorera la réactivité du système.  
5. **Surveiller les Ressources Système :** Une surveillance attentive de l'utilisation de la RAM et du CPU sera cruciale, en particulier pour le service Ollama. Des ajustements dans la taille du modèle LLM ou la configuration de la quantification pourraient être nécessaires pour maintenir des performances acceptables.  
6. **Sécurité et Accès :** Assurer la sécurité de l'API Paperless-ngx et de l'instance Ollama, surtout si le système est exposé au réseau. Utiliser des clés API robustes et envisager un reverse proxy pour une couche de sécurité supplémentaire.  
7. **Itérer et Évaluer :** Commencer par une implémentation minimale viable, puis itérer en fonction des besoins. Évaluer régulièrement la pertinence des résultats de recherche et la qualité des réponses du LLM pour affiner les configurations et les modèles.

En suivant ces recommandations, il est possible de construire un système de gestion de documents intelligent et privé, capable de fournir des réponses précises et contextuelles, même dans un environnement matériel contraint.

#### **Sources des citations**

1. Neural search: Definition, how it works, benefits and more \- Meilisearch, consulté le juillet 20, 2025, [https://www.meilisearch.com/blog/neural-search](https://www.meilisearch.com/blog/neural-search)  
2. What is Neural Search and How Does It Work? \- Moveworks, consulté le juillet 20, 2025, [https://www.moveworks.com/us/en/resources/blog/what-is-neural-search](https://www.moveworks.com/us/en/resources/blog/what-is-neural-search)  
3. What is Retrieval Augmented Generation (RAG) for LLMs? \- Hopsworks, consulté le juillet 20, 2025, [https://www.hopsworks.ai/dictionary/retrieval-augmented-generation-llm](https://www.hopsworks.ai/dictionary/retrieval-augmented-generation-llm)  
4. What is RAG? \- Retrieval-Augmented Generation AI Explained \- AWS, consulté le juillet 20, 2025, [https://aws.amazon.com/what-is/retrieval-augmented-generation/](https://aws.amazon.com/what-is/retrieval-augmented-generation/)  
5. Paperless-AI on Synology NAS: AI-Powered Document Management with Paperless-NGX, consulté le juillet 20, 2025, [https://blog.admin-intelligence.de/en/paperless-ai-on-synology-nas-ai-powered-document-management-with-paperless-ngx/](https://blog.admin-intelligence.de/en/paperless-ai-on-synology-nas-ai-powered-document-management-with-paperless-ngx/)  
6. Paperless-ngx, consulté le juillet 20, 2025, [https://docs.paperless-ngx.com/](https://docs.paperless-ngx.com/)  
7. Features \- Vespa Documentation, consulté le juillet 20, 2025, [https://docs.vespa.ai/en/features.html](https://docs.vespa.ai/en/features.html)  
8. Vespa.ai \- Vespa.ai, consulté le juillet 20, 2025, [https://vespa.ai/](https://vespa.ai/)  
9. Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction ..., consulté le juillet 20, 2025, [http://arxiv.org/pdf/2408.16672](http://arxiv.org/pdf/2408.16672)  
10. Working with ColBERT \- Qdrant, consulté le juillet 20, 2025, [https://qdrant.tech/documentation/fastembed/fastembed-colbert/](https://qdrant.tech/documentation/fastembed/fastembed-colbert/)  
11. Accelerating LLM Inference with OLAMA: A GPU-Enabled Approach \- GoPenAI, consulté le juillet 20, 2025, [https://blog.gopenai.com/accelerating-llm-inference-with-olama-a-gpu-enabled-approach-bc1cc27dd246](https://blog.gopenai.com/accelerating-llm-inference-with-olama-a-gpu-enabled-approach-bc1cc27dd246)  
12. Using Ollama to host an LLM on CPU-only equipment to enable a local chatbot and LLM API, consulté le juillet 20, 2025, [https://blog.gordonbuchan.com/blog/index.php/2025/01/11/using-ollama-to-host-an-llm-on-cpu-only-equipment-to-enable-a-local-chatbot-and-llm-api-server/](https://blog.gordonbuchan.com/blog/index.php/2025/01/11/using-ollama-to-host-an-llm-on-cpu-only-equipment-to-enable-a-local-chatbot-and-llm-api-server/)  
13. Setting Up Ollama with Open-WebUI: A Docker Compose Guide \- Archy.net, consulté le juillet 20, 2025, [https://www.archy.net/setting-up-ollama-with-open-webui-a-docker-compose-guide/](https://www.archy.net/setting-up-ollama-with-open-webui-a-docker-compose-guide/)  
14. How to use Ollama and Open WebUI with Docker Compose \[Part 4\] \- Geshan Manandhar, consulté le juillet 20, 2025, [https://geshan.com.np/blog/2025/02/ollama-docker-compose/](https://geshan.com.np/blog/2025/02/ollama-docker-compose/)  
15. What is ColBERT and how does it differ from standard bi-encoder approaches? \- Milvus, consulté le juillet 20, 2025, [https://milvus.io/ai-quick-reference/what-is-colbert-and-how-does-it-differ-from-standard-biencoder-approaches](https://milvus.io/ai-quick-reference/what-is-colbert-and-how-does-it-differ-from-standard-biencoder-approaches)  
16. ColBERT \+ PaliGemma \= ColiPali for Document Retrieval | by Samvardhan V G | Medium, consulté le juillet 20, 2025, [https://medium.com/@samvardhan777/colbert-paligemma-colipali-for-document-retrieval-3bb8cc80326c](https://medium.com/@samvardhan777/colbert-paligemma-colipali-for-document-retrieval-3bb8cc80326c)  
17. How the ColBERT re-ranker model in a RAG system works \- IBM Developer, consulté le juillet 20, 2025, [https://developer.ibm.com/articles/how-colbert-works/](https://developer.ibm.com/articles/how-colbert-works/)  
18. Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever \- arXiv, consulté le juillet 20, 2025, [https://arxiv.org/html/2408.16672v4](https://arxiv.org/html/2408.16672v4)  
19. RAGatouille | 🦜️ LangChain, consulté le juillet 20, 2025, [https://python.langchain.com/docs/integrations/providers/ragatouille/](https://python.langchain.com/docs/integrations/providers/ragatouille/)  
20. Revolutionizing Semantic Search with Multi-Vector HNSW Indexing in Vespa, consulté le juillet 20, 2025, [https://blog.vespa.ai/semantic-search-with-multi-vector-indexing/](https://blog.vespa.ai/semantic-search-with-multi-vector-indexing/)  
21. Ranking \- Vespa Documentation, consulté le juillet 20, 2025, [https://docs.vespa.ai/en/ranking.html](https://docs.vespa.ai/en/ranking.html)  
22. getting-started-ranking.md \- vespa-engine/documentation \- GitHub, consulté le juillet 20, 2025, [https://github.com/vespa-engine/documentation/blob/master/en/getting-started-ranking.md](https://github.com/vespa-engine/documentation/blob/master/en/getting-started-ranking.md)  
23. Paperless-ngx \- Quickstart | Elest.io, consulté le juillet 20, 2025, [https://elest.io/open-source/paperless-ngx/resources/quickstart](https://elest.io/open-source/paperless-ngx/resources/quickstart)  
24. Advanced Topics \- Paperless-ngx, consulté le juillet 20, 2025, [https://docs.paperless-ngx.com/advanced\_usage/](https://docs.paperless-ngx.com/advanced_usage/)  
25. Today's Deep-Dive: Paperless-ngx \- Podcasts safeserver.de, consulté le juillet 20, 2025, [https://podcast.safeserver.de/@deepdive/episodes/todays-deep-dive-paperless-ngx/transcript](https://podcast.safeserver.de/@deepdive/episodes/todays-deep-dive-paperless-ngx/transcript)  
26. Usage Overview \- Paperless-ngx, consulté le juillet 20, 2025, [https://docs.paperless-ngx.com/usage/](https://docs.paperless-ngx.com/usage/)  
27. REST API \- Paperless-ngx, consulté le juillet 20, 2025, [https://docs.paperless-ngx.com/api/](https://docs.paperless-ngx.com/api/)  
28. Paperless-ngx \- Home Assistant, consulté le juillet 20, 2025, [https://www.home-assistant.io/integrations/paperless\_ngx/](https://www.home-assistant.io/integrations/paperless_ngx/)  
29. Administration \- Paperless-ngx, consulté le juillet 20, 2025, [https://docs.paperless-ngx.com/administration/](https://docs.paperless-ngx.com/administration/)  
30. Ollama: Run quantized LLMs on CPUs and GPUs — SkyPilot documentation, consulté le juillet 20, 2025, [https://docs.skypilot.co/en/v0.9.2/examples/serving/ollama.html](https://docs.skypilot.co/en/v0.9.2/examples/serving/ollama.html)  
31. Ollama CPU: Model Manager Script & Inference with a Terminal UI \- YouTube, consulté le juillet 20, 2025, [https://www.youtube.com/watch?v=f\_M4WfxJ4mQ](https://www.youtube.com/watch?v=f_M4WfxJ4mQ)  
32. Running Small LLMs Locally: My Journey With and Without GPUs | by Ibrahim Sajid Malick, consulté le juillet 20, 2025, [https://medium.com/@IbrahimMalick/running-small-llms-locally-my-journey-with-and-without-gpus-1e256cde33bb](https://medium.com/@IbrahimMalick/running-small-llms-locally-my-journey-with-and-without-gpus-1e256cde33bb)  
33. seeking a fast local LLM solution using only CPU : r/ollama \- Reddit, consulté le juillet 20, 2025, [https://www.reddit.com/r/ollama/comments/1ir69xk/seeking\_a\_fast\_local\_llm\_solution\_using\_only\_cpu/](https://www.reddit.com/r/ollama/comments/1ir69xk/seeking_a_fast_local_llm_solution_using_only_cpu/)  
34. Using Quantized Models with Ollama for Application Development \- MachineLearningMastery.com, consulté le juillet 20, 2025, [https://machinelearningmastery.com/using-quantized-models-with-ollama-for-application-development/](https://machinelearningmastery.com/using-quantized-models-with-ollama-for-application-development/)  
35. 10 Best Small Local LLMs to Try Out (\< 8GB) \- Apidog, consulté le juillet 20, 2025, [https://apidog.com/blog/small-local-llm/](https://apidog.com/blog/small-local-llm/)  
36. Preventing Model Swapping In Ollama — A Guide To Persistent Loading \- GoPenAI, consulté le juillet 20, 2025, [https://blog.gopenai.com/preventing-model-swapping-in-ollama-a-guide-to-persistent-loading-f81f1dfb858d](https://blog.gopenai.com/preventing-model-swapping-in-ollama-a-guide-to-persistent-loading-f81f1dfb858d)  
37. Configuration \- Paperless-ngx, consulté le juillet 20, 2025, [https://docs.paperless-ngx.com/configuration/](https://docs.paperless-ngx.com/configuration/)  
38. How to Use Ollama (Complete Ollama Cheatsheet) \- Apidog, consulté le juillet 20, 2025, [https://apidog.com/blog/how-to-use-ollama/](https://apidog.com/blog/how-to-use-ollama/)  
39. Networking With Docker Compose (Quick Guide) \- Netmaker, consulté le juillet 20, 2025, [https://www.netmaker.io/resources/docker-compose-network](https://www.netmaker.io/resources/docker-compose-network)  
40. Communication between multiple docker-compose projects, consulté le juillet 20, 2025, [https://dev.to/iamrj846/communication-between-multiple-docker-compose-projects-223k](https://dev.to/iamrj846/communication-between-multiple-docker-compose-projects-223k)  
41. vespaengine/vespa \- Docker Image, consulté le juillet 20, 2025, [https://hub.docker.com/r/vespaengine/vespa/](https://hub.docker.com/r/vespaengine/vespa/)  
42. docker-compose.yaml \- vespa-engine/sample-apps \- GitHub, consulté le juillet 20, 2025, [https://github.com/vespa-engine/sample-apps/blob/master/examples/operations/multinode-HA/docker-compose.yaml](https://github.com/vespa-engine/sample-apps/blob/master/examples/operations/multinode-HA/docker-compose.yaml)  
43. paperless-ngx-compose/docker-compose.env at main \- GitHub, consulté le juillet 20, 2025, [https://github.com/BWibo/paperless-ngx-compose/blob/main/docker-compose.env](https://github.com/BWibo/paperless-ngx-compose/blob/main/docker-compose.env)#   n a v i s d o c s . a i  
 