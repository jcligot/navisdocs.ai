<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=DFQxm4rd7fRHgM9OTejWVT5Vho6BE7M80rHXEVKqXWegg2XYR88pwOsaJkfiF7cJu5e0vFtnyLdhsxviZUUN-U0KZVwUvSK-LyXz4qcE1hc);.lst-kix_list_c-0>li:before{content:"" counter(lst-ctn-kix_list_c-0,decimal) ". "}ul.lst-kix_list_b-8{list-style-type:none}ul.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_c-1>li:before{content:"\0025cb   "}ul.lst-kix_list_b-3{list-style-type:none}ul.lst-kix_list_b-2{list-style-type:none}.lst-kix_list_c-3>li:before{content:"\0025a0   "}.lst-kix_list_c-4>li:before{content:"\0025a0   "}ul.lst-kix_list_b-1{list-style-type:none}ul.lst-kix_list_b-7{list-style-type:none}ul.lst-kix_list_b-6{list-style-type:none}.lst-kix_list_c-2>li:before{content:"\0025a0   "}ul.lst-kix_list_b-5{list-style-type:none}ul.lst-kix_list_b-4{list-style-type:none}ul.lst-kix_list_9-3{list-style-type:none}ul.lst-kix_list_9-4{list-style-type:none}ul.lst-kix_list_9-1{list-style-type:none}ul.lst-kix_list_9-2{list-style-type:none}ul.lst-kix_list_9-7{list-style-type:none}ul.lst-kix_list_9-8{list-style-type:none}ul.lst-kix_list_9-5{list-style-type:none}ul.lst-kix_list_9-6{list-style-type:none}ul.lst-kix_list_1-3{list-style-type:none}ul.lst-kix_list_1-4{list-style-type:none}ul.lst-kix_list_1-1{list-style-type:none}ul.lst-kix_list_1-2{list-style-type:none}ul.lst-kix_list_1-7{list-style-type:none}ul.lst-kix_list_9-0{list-style-type:none}ul.lst-kix_list_1-8{list-style-type:none}ul.lst-kix_list_1-5{list-style-type:none}ul.lst-kix_list_1-6{list-style-type:none}.lst-kix_list_5-0>li:before{content:"\0025cf   "}.lst-kix_list_5-3>li:before{content:"\0025a0   "}ul.lst-kix_list_a-4{list-style-type:none}ul.lst-kix_list_a-3{list-style-type:none}.lst-kix_list_5-2>li:before{content:"\0025a0   "}ul.lst-kix_list_a-2{list-style-type:none}ul.lst-kix_list_a-1{list-style-type:none}.lst-kix_list_5-1>li:before{content:"\0025cb   "}ul.lst-kix_list_a-8{list-style-type:none}ul.lst-kix_list_a-7{list-style-type:none}ul.lst-kix_list_a-6{list-style-type:none}ul.lst-kix_list_a-5{list-style-type:none}.lst-kix_list_5-7>li:before{content:"\0025a0   "}ul.lst-kix_list_8-4{list-style-type:none}ul.lst-kix_list_8-5{list-style-type:none}.lst-kix_list_5-6>li:before{content:"\0025a0   "}.lst-kix_list_5-8>li:before{content:"\0025a0   "}ul.lst-kix_list_8-2{list-style-type:none}ul.lst-kix_list_8-3{list-style-type:none}ul.lst-kix_list_8-8{list-style-type:none}ul.lst-kix_list_8-6{list-style-type:none}ul.lst-kix_list_8-7{list-style-type:none}.lst-kix_list_d-6>li:before{content:"\0025a0   "}.lst-kix_list_5-4>li:before{content:"\0025a0   "}.lst-kix_list_5-5>li:before{content:"\0025a0   "}ul.lst-kix_list_8-0{list-style-type:none}.lst-kix_list_d-8>li:before{content:"\0025a0   "}ul.lst-kix_list_8-1{list-style-type:none}.lst-kix_list_d-7>li:before{content:"\0025a0   "}.lst-kix_list_c-0>li{counter-increment:lst-ctn-kix_list_c-0}.lst-kix_list_d-2>li:before{content:"\0025a0   "}.lst-kix_list_6-1>li:before{content:"\0025cb   "}.lst-kix_list_6-3>li:before{content:"\0025a0   "}.lst-kix_list_d-1>li:before{content:"\0025cb   "}.lst-kix_list_d-5>li:before{content:"\0025a0   "}.lst-kix_list_6-0>li:before{content:"\0025cf   "}.lst-kix_list_6-4>li:before{content:"\0025a0   "}.lst-kix_list_d-4>li:before{content:"\0025a0   "}.lst-kix_list_d-3>li:before{content:"\0025a0   "}.lst-kix_list_6-2>li:before{content:"\0025a0   "}.lst-kix_list_c-7>li:before{content:"\0025a0   "}ol.lst-kix_list_d-0.start{counter-reset:lst-ctn-kix_list_d-0 0}.lst-kix_list_c-5>li:before{content:"\0025a0   "}.lst-kix_list_c-8>li:before{content:"\0025a0   "}.lst-kix_list_6-8>li:before{content:"\0025a0   "}.lst-kix_list_6-5>li:before{content:"\0025a0   "}.lst-kix_list_6-7>li:before{content:"\0025a0   "}.lst-kix_list_d-0>li:before{content:"" counter(lst-ctn-kix_list_d-0,decimal) ". "}.lst-kix_list_c-6>li:before{content:"\0025a0   "}.lst-kix_list_6-6>li:before{content:"\0025a0   "}.lst-kix_list_2-7>li:before{content:"\0025a0   "}ul.lst-kix_list_d-8{list-style-type:none}ul.lst-kix_list_d-7{list-style-type:none}.lst-kix_list_7-4>li:before{content:"\0025a0   "}.lst-kix_list_7-6>li:before{content:"\0025a0   "}ul.lst-kix_list_d-6{list-style-type:none}.lst-kix_list_2-5>li:before{content:"\0025a0   "}ul.lst-kix_list_d-1{list-style-type:none}.lst-kix_list_b-8>li:before{content:"\0025a0   "}.lst-kix_list_7-2>li:before{content:"\0025a0   "}ul.lst-kix_list_d-5{list-style-type:none}ul.lst-kix_list_d-4{list-style-type:none}ul.lst-kix_list_d-3{list-style-type:none}ul.lst-kix_list_d-2{list-style-type:none}.lst-kix_list_b-4>li:before{content:"\0025a0   "}ul.lst-kix_list_3-7{list-style-type:none}ul.lst-kix_list_3-8{list-style-type:none}ul.lst-kix_list_3-1{list-style-type:none}ul.lst-kix_list_3-2{list-style-type:none}.lst-kix_list_7-8>li:before{content:"\0025a0   "}ul.lst-kix_list_3-0{list-style-type:none}ul.lst-kix_list_3-5{list-style-type:none}ul.lst-kix_list_3-6{list-style-type:none}.lst-kix_list_b-6>li:before{content:"\0025a0   "}ul.lst-kix_list_3-3{list-style-type:none}ul.lst-kix_list_3-4{list-style-type:none}.lst-kix_list_b-0>li:before{content:"" counter(lst-ctn-kix_list_b-0,decimal) ". "}.lst-kix_list_4-1>li:before{content:"\0025cb   "}.lst-kix_list_b-2>li:before{content:"\0025a0   "}.lst-kix_list_9-2>li:before{content:"\0025a0   "}.lst-kix_list_4-3>li:before{content:"\0025a0   "}.lst-kix_list_4-5>li:before{content:"\0025a0   "}ol.lst-kix_list_e-0.start{counter-reset:lst-ctn-kix_list_e-0 0}ol.lst-kix_list_b-0{list-style-type:none}.lst-kix_list_9-0>li:before{content:"\0025cf   "}ul.lst-kix_list_c-8{list-style-type:none}ul.lst-kix_list_c-7{list-style-type:none}.lst-kix_list_9-6>li:before{content:"\0025a0   "}ul.lst-kix_list_c-2{list-style-type:none}ul.lst-kix_list_c-1{list-style-type:none}.lst-kix_list_9-4>li:before{content:"\0025a0   "}ul.lst-kix_list_c-6{list-style-type:none}.lst-kix_list_a-0>li:before{content:"" counter(lst-ctn-kix_list_a-0,decimal) ". "}ul.lst-kix_list_c-5{list-style-type:none}ul.lst-kix_list_c-4{list-style-type:none}ul.lst-kix_list_c-3{list-style-type:none}ul.lst-kix_list_2-8{list-style-type:none}ol.lst-kix_list_b-0.start{counter-reset:lst-ctn-kix_list_b-0 0}ul.lst-kix_list_2-2{list-style-type:none}ul.lst-kix_list_2-3{list-style-type:none}ul.lst-kix_list_2-0{list-style-type:none}ul.lst-kix_list_2-1{list-style-type:none}.lst-kix_list_9-8>li:before{content:"\0025a0   "}ul.lst-kix_list_2-6{list-style-type:none}.lst-kix_list_1-1>li:before{content:"\0025cb   "}ul.lst-kix_list_2-7{list-style-type:none}ul.lst-kix_list_2-4{list-style-type:none}ul.lst-kix_list_2-5{list-style-type:none}.lst-kix_list_e-0>li{counter-increment:lst-ctn-kix_list_e-0}.lst-kix_list_b-0>li{counter-increment:lst-ctn-kix_list_b-0}.lst-kix_list_1-3>li:before{content:"\0025a0   "}.lst-kix_list_1-7>li:before{content:"\0025a0   "}.lst-kix_list_1-5>li:before{content:"\0025a0   "}.lst-kix_list_2-1>li:before{content:"\0025cb   "}ol.lst-kix_list_a-0{list-style-type:none}.lst-kix_list_2-3>li:before{content:"\0025a0   "}.lst-kix_list_3-0>li:before{content:"\0025cf   "}ul.lst-kix_list_5-7{list-style-type:none}ul.lst-kix_list_5-8{list-style-type:none}.lst-kix_list_3-1>li:before{content:"\0025cb   "}.lst-kix_list_3-2>li:before{content:"\0025a0   "}ul.lst-kix_list_5-5{list-style-type:none}ol.lst-kix_list_c-0.start{counter-reset:lst-ctn-kix_list_c-0 0}ul.lst-kix_list_5-6{list-style-type:none}.lst-kix_list_8-1>li:before{content:"\0025cb   "}.lst-kix_list_8-2>li:before{content:"\0025a0   "}.lst-kix_list_3-5>li:before{content:"\0025a0   "}ul.lst-kix_list_5-0{list-style-type:none}.lst-kix_list_3-4>li:before{content:"\0025a0   "}ul.lst-kix_list_5-3{list-style-type:none}.lst-kix_list_3-3>li:before{content:"\0025a0   "}ul.lst-kix_list_5-4{list-style-type:none}ul.lst-kix_list_5-1{list-style-type:none}.lst-kix_list_8-0>li:before{content:"\0025cf   "}ul.lst-kix_list_5-2{list-style-type:none}.lst-kix_list_8-7>li:before{content:"\0025a0   "}.lst-kix_list_3-8>li:before{content:"\0025a0   "}.lst-kix_list_8-5>li:before{content:"\0025a0   "}.lst-kix_list_a-7>li:before{content:"\0025a0   "}.lst-kix_list_8-6>li:before{content:"\0025a0   "}.lst-kix_list_8-3>li:before{content:"\0025a0   "}.lst-kix_list_3-6>li:before{content:"\0025a0   "}.lst-kix_list_3-7>li:before{content:"\0025a0   "}.lst-kix_list_a-8>li:before{content:"\0025a0   "}.lst-kix_list_8-4>li:before{content:"\0025a0   "}ol.lst-kix_list_d-0{list-style-type:none}.lst-kix_list_a-1>li:before{content:"\0025cb   "}.lst-kix_list_a-2>li:before{content:"\0025a0   "}.lst-kix_list_a-3>li:before{content:"\0025a0   "}.lst-kix_list_a-5>li:before{content:"\0025a0   "}.lst-kix_list_a-6>li:before{content:"\0025a0   "}.lst-kix_list_a-4>li:before{content:"\0025a0   "}.lst-kix_list_8-8>li:before{content:"\0025a0   "}.lst-kix_list_e-5>li:before{content:"\0025a0   "}ul.lst-kix_list_e-8{list-style-type:none}ul.lst-kix_list_e-7{list-style-type:none}ul.lst-kix_list_e-6{list-style-type:none}ul.lst-kix_list_e-5{list-style-type:none}.lst-kix_list_e-3>li:before{content:"\0025a0   "}.lst-kix_list_e-7>li:before{content:"\0025a0   "}.lst-kix_list_e-2>li:before{content:"\0025a0   "}.lst-kix_list_e-6>li:before{content:"\0025a0   "}.lst-kix_list_4-8>li:before{content:"\0025a0   "}.lst-kix_list_4-7>li:before{content:"\0025a0   "}ul.lst-kix_list_e-4{list-style-type:none}ul.lst-kix_list_e-3{list-style-type:none}ul.lst-kix_list_e-2{list-style-type:none}.lst-kix_list_e-4>li:before{content:"\0025a0   "}ul.lst-kix_list_e-1{list-style-type:none}ul.lst-kix_list_4-8{list-style-type:none}ul.lst-kix_list_4-6{list-style-type:none}ul.lst-kix_list_4-7{list-style-type:none}ul.lst-kix_list_4-0{list-style-type:none}.lst-kix_list_e-1>li:before{content:"\0025cb   "}ul.lst-kix_list_4-1{list-style-type:none}ul.lst-kix_list_4-4{list-style-type:none}ul.lst-kix_list_4-5{list-style-type:none}ul.lst-kix_list_4-2{list-style-type:none}.lst-kix_list_e-0>li:before{content:"" counter(lst-ctn-kix_list_e-0,decimal) ". "}ul.lst-kix_list_4-3{list-style-type:none}.lst-kix_list_d-0>li{counter-increment:lst-ctn-kix_list_d-0}ol.lst-kix_list_c-0{list-style-type:none}.lst-kix_list_7-0>li:before{content:"\0025cf   "}.lst-kix_list_2-6>li:before{content:"\0025a0   "}.lst-kix_list_2-4>li:before{content:"\0025a0   "}.lst-kix_list_2-8>li:before{content:"\0025a0   "}.lst-kix_list_7-1>li:before{content:"\0025cb   "}.lst-kix_list_7-5>li:before{content:"\0025a0   "}.lst-kix_list_7-3>li:before{content:"\0025a0   "}ul.lst-kix_list_7-5{list-style-type:none}ul.lst-kix_list_7-6{list-style-type:none}.lst-kix_list_b-5>li:before{content:"\0025a0   "}ul.lst-kix_list_7-3{list-style-type:none}ul.lst-kix_list_7-4{list-style-type:none}.lst-kix_list_b-3>li:before{content:"\0025a0   "}.lst-kix_list_b-7>li:before{content:"\0025a0   "}ul.lst-kix_list_7-7{list-style-type:none}ul.lst-kix_list_7-8{list-style-type:none}ol.lst-kix_list_a-0.start{counter-reset:lst-ctn-kix_list_a-0 0}ul.lst-kix_list_7-1{list-style-type:none}ul.lst-kix_list_7-2{list-style-type:none}ul.lst-kix_list_7-0{list-style-type:none}.lst-kix_list_7-7>li:before{content:"\0025a0   "}.lst-kix_list_4-0>li:before{content:"\0025cf   "}.lst-kix_list_b-1>li:before{content:"\0025cb   "}.lst-kix_list_4-4>li:before{content:"\0025a0   "}.lst-kix_list_4-2>li:before{content:"\0025a0   "}.lst-kix_list_4-6>li:before{content:"\0025a0   "}.lst-kix_list_9-3>li:before{content:"\0025a0   "}.lst-kix_list_9-1>li:before{content:"\0025cb   "}.lst-kix_list_e-8>li:before{content:"\0025a0   "}.lst-kix_list_9-7>li:before{content:"\0025a0   "}.lst-kix_list_9-5>li:before{content:"\0025a0   "}ul.lst-kix_list_6-6{list-style-type:none}ul.lst-kix_list_6-7{list-style-type:none}ul.lst-kix_list_6-4{list-style-type:none}ul.lst-kix_list_6-5{list-style-type:none}ul.lst-kix_list_6-8{list-style-type:none}.lst-kix_list_1-0>li:before{content:"\0025cf   "}ul.lst-kix_list_6-2{list-style-type:none}.lst-kix_list_a-0>li{counter-increment:lst-ctn-kix_list_a-0}ul.lst-kix_list_6-3{list-style-type:none}.lst-kix_list_1-2>li:before{content:"\0025a0   "}ul.lst-kix_list_6-0{list-style-type:none}ul.lst-kix_list_6-1{list-style-type:none}.lst-kix_list_1-4>li:before{content:"\0025a0   "}.lst-kix_list_1-6>li:before{content:"\0025a0   "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ol.lst-kix_list_e-0{list-style-type:none}.lst-kix_list_2-0>li:before{content:"\0025cf   "}.lst-kix_list_1-8>li:before{content:"\0025a0   "}.lst-kix_list_2-2>li:before{content:"\0025a0   "}ol{margin:0;padding:0}table td,table th{padding:0}.c2{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:117pt;border-top-color:#000000;border-bottom-style:solid}.c3{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Google Sans Text";font-style:normal}.c5{-webkit-text-decoration-skip:none;color:#0000ee;font-weight:400;text-decoration:underline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Google Sans"}.c6{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Google Sans";font-style:normal}.c26{color:#000000;font-weight:700;text-decoration:none;font-size:18pt;font-family:"Google Sans"}.c27{color:#000000;font-weight:700;text-decoration:none;font-size:24pt;font-family:"Google Sans"}.c7{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c29{color:#000000;font-weight:700;text-decoration:none;font-size:12pt;font-family:"Google Sans"}.c13{color:#000000;text-decoration:none;vertical-align:super;font-size:12pt;font-style:normal}.c19{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c17{padding-top:0pt;padding-bottom:11.2pt;line-height:1.149999976158142;text-align:left}.c25{border-spacing:0;border-collapse:collapse;margin-right:auto}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.149999976158142;text-align:left}.c22{padding-top:0pt;padding-bottom:12pt;line-height:1.149999976158142;text-align:left}.c28{padding-top:24pt;padding-bottom:0pt;line-height:1.149999976158142;text-align:left}.c23{padding-top:0pt;padding-bottom:12.8pt;line-height:1.149999976158142;text-align:left}.c32{padding-top:0pt;padding-bottom:12.8pt;line-height:1.0;text-align:left}.c15{font-size:12pt;font-weight:400;font-family:"Google Sans"}.c30{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c1{padding:0;margin:0}.c21{font-weight:400;font-family:"Google Sans"}.c9{vertical-align:baseline;font-style:normal}.c8{font-weight:700;font-family:"Google Sans Text"}.c31{font-weight:400;font-family:"Arial"}.c14{vertical-align:super;font-size:12pt}.c0{font-weight:400;font-family:"Google Sans Text"}.c16{margin-left:90pt;padding-left:0pt}.c18{margin-left:60pt;padding-left:0pt}.c11{color:inherit;text-decoration:inherit}.c12{margin-left:30pt;padding-left:0pt}.c24{height:0pt}.c10{height:11pt}.c20{font-style:italic}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.0;page-break-after:avoid;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.0;page-break-after:avoid;font-style:italic;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:12pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:12pt;font-family:"Arial";line-height:1.0;text-align:left}h2{padding-top:11.2pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:11.2pt;font-family:"Arial";line-height:1.0;text-align:left}h3{padding-top:12pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:12pt;font-family:"Arial";line-height:1.0;text-align:left}h4{padding-top:12.8pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:12.8pt;font-family:"Arial";line-height:1.0;text-align:left}h5{padding-top:12.8pt;color:#000000;font-weight:700;font-size:9pt;padding-bottom:12.8pt;font-family:"Arial";line-height:1.0;text-align:left}h6{padding-top:18pt;color:#000000;font-weight:700;font-size:8pt;padding-bottom:18pt;font-family:"Arial";line-height:1.0;text-align:left}</style></head><body class="c30 doc-content"><p class="c4 c10"><span class="c7 c31"></span></p><h1 class="c22"><span class="c9 c27"> Mise en &OElig;uvre d&#39;un Syst&egrave;me de Recherche Neurale de Nouvelle G&eacute;n&eacute;ration avec ColBERT (Interaction Tardive) et Vespa, Int&eacute;gr&eacute; &agrave; Paperless-ngx AI et Ollama pour Mod&egrave;les LLM sur CPU, via Docker Compose</span></h1><p class="c4 c10"><span class="c7 c21"></span></p><p class="c4 c10"><span class="c7 c21"></span></p><h2 class="c17"><span class="c26 c9">1. Introduction : Le Paysage de la Recherche Neurale et de la G&eacute;n&eacute;ration Augment&eacute;e par la R&eacute;cup&eacute;ration (RAG) de Nouvelle G&eacute;n&eacute;ration</span></h2><p class="c4 c10"><span class="c7 c21"></span></p><p class="c4"><span class="c0">La recherche neurale, ou r&eacute;cup&eacute;ration d&#39;informations neuronales, repr&eacute;sente une transformation fondamentale dans le domaine de la recherche d&#39;informations. Elle d&eacute;passe les approches traditionnelles bas&eacute;es sur les mots-cl&eacute;s et m&ecirc;me les m&eacute;thodes de recherche vectorielle de base. En exploitant des r&eacute;seaux de neurones profonds (DNN), la recherche neurale interpr&egrave;te la signification contextuelle et les relations s&eacute;mantiques inh&eacute;rentes aux requ&ecirc;tes des utilisateurs et aux donn&eacute;es. Cette compr&eacute;hension avanc&eacute;e aboutit &agrave; des r&eacute;sultats plus pr&eacute;cis, adaptables et pertinents, et prend en charge de mani&egrave;re cruciale les donn&eacute;es multimodales (texte, images, audio et ensembles de donn&eacute;es 3D complexes), ce qui est essentiel pour les applications bas&eacute;es sur l&#39;IA et les exp&eacute;riences utilisateur de nouvelle g&eacute;n&eacute;ration.</span><span class="c13 c0">1</span></p><p class="c4"><span class="c0">Une distinction cl&eacute; de la recherche neurale r&eacute;side dans son utilisation des DNN tout au long du pipeline de recherche, de l&#39;encodage et de l&#39;indexation des donn&eacute;es &agrave; l&#39;encodage des requ&ecirc;tes et aux m&eacute;canismes sophistiqu&eacute;s de correspondance s&eacute;mantique et de classement. Cette application holistique de l&#39;apprentissage profond permet un apprentissage continu et un affinement des r&eacute;sultats, surpassant constamment les m&eacute;thodes de recherche conventionnelles en termes de pr&eacute;cision et d&#39;adaptabilit&eacute;.</span><span class="c14 c0">1</span><span class="c0">&nbsp;Le processus fondamental implique la transformation de l&#39;entr&eacute;e de l&#39;utilisateur et des donn&eacute;es sous-jacentes en plongements vectoriels denses &ndash; des repr&eacute;sentations num&eacute;riques qui capturent efficacement les relations s&eacute;mantiques entre les concepts. Le DNN compare ensuite directement ces plongements, &eacute;valuant les relations apprises pour classer les r&eacute;sultats en fonction de leur alignement contextuel, d&eacute;passant ainsi les limites de la correspondance exacte des termes.</span><span class="c13 c0">1</span></p><p class="c4"><span class="c0">La G&eacute;n&eacute;ration Augment&eacute;e par la R&eacute;cup&eacute;ration (RAG) est une technique d&#39;optimisation pour les grands mod&egrave;les de langage (LLM) qui am&eacute;liore consid&eacute;rablement la qualit&eacute; des pr&eacute;dictions. Elle permet aux LLM de r&eacute;f&eacute;rencer une base de connaissances externe et faisant autorit&eacute;, en dehors de leurs sources de donn&eacute;es d&#39;entra&icirc;nement d&#39;origine, pendant la phase d&#39;inf&eacute;rence. Cette approche aborde directement les limites courantes des LLM, telles que les inexactitudes factuelles (souvent appel&eacute;es &quot;hallucinations&quot;), les informations obsol&egrave;tes et un manque g&eacute;n&eacute;ral de connaissances sp&eacute;cifiques &agrave; un domaine.</span><span class="c13 c0">3</span></p><p class="c4"><span class="c0">Le processus RAG introduit un composant de r&eacute;cup&eacute;ration d&#39;informations qui utilise la requ&ecirc;te d&#39;entr&eacute;e de l&#39;utilisateur pour extraire d&#39;abord des informations de &quot;contexte&quot; pertinentes d&#39;un magasin de donn&eacute;es d&eacute;sign&eacute;, g&eacute;n&eacute;ralement une base de donn&eacute;es vectorielle. Ces informations pertinentes r&eacute;cup&eacute;r&eacute;es sont ensuite combin&eacute;es intelligemment avec la requ&ecirc;te originale de l&#39;utilisateur pour construire une requ&ecirc;te plus riche et mieux inform&eacute;e. Le LLM utilise ensuite cette requ&ecirc;te augment&eacute;e, tirant parti des nouvelles connaissances ainsi que de ses donn&eacute;es d&#39;entra&icirc;nement, pour g&eacute;n&eacute;rer des r&eacute;ponses plus pr&eacute;cises, pertinentes et fond&eacute;es.</span><span class="c14 c0">3</span><span class="c0">&nbsp;Le RAG offre des avantages substantiels : il peut atteindre une qualit&eacute; de pr&eacute;diction sup&eacute;rieure m&ecirc;me avec des LLM ayant moins de param&egrave;tres, permet des mises &agrave; jour dynamiques des connaissances simplement en actualisant les corpus de r&eacute;cup&eacute;ration, et fournit des citations v&eacute;rifiables pour que les utilisateurs puissent facilement &eacute;valuer et faire confiance aux pr&eacute;dictions g&eacute;n&eacute;r&eacute;es. Il est particuli&egrave;rement efficace pour incorporer des donn&eacute;es en temps r&eacute;el, des donn&eacute;es utilisateur personnelles ou des informations sp&eacute;cifiques au contexte qui n&#39;&eacute;taient pas disponibles pour le LLM au moment de son entra&icirc;nement.</span><span class="c13 c0">3</span></p><p class="c4 c10"><span class="c13 c0"></span></p><h3 class="c22"><span class="c6">Pr&eacute;sentation de l&#39;Architecture Propos&eacute;e : ColBERT, Vespa, Paperless-ngx AI, Ollama et Docker Compose</span></h3><p class="c4 c10"><span class="c7 c21"></span></p><p class="c4"><span class="c7 c0">Ce rapport d&eacute;taille la construction d&#39;un syst&egrave;me complet et auto-h&eacute;berg&eacute; de G&eacute;n&eacute;ration Augment&eacute;e par la R&eacute;cup&eacute;ration (RAG), sp&eacute;cifiquement con&ccedil;u pour la compr&eacute;hension et la r&eacute;cup&eacute;ration intelligente de documents.</span></p><ul class="c1 lst-kix_list_1-0 start"><li class="c4 c12 li-bullet-0"><span class="c8">Paperless-ngx AI</span><span class="c0">&nbsp;fonctionnera comme la couche principale d&#39;ingestion et de gestion des documents. Il g&eacute;rera la reconnaissance optique de caract&egrave;res (OCR) pour les documents num&eacute;ris&eacute;s, effectue l&#39;extraction initiale des m&eacute;tadonn&eacute;es et maintiendra une archive structur&eacute;e et consultable de documents.</span><span class="c13 c0">5</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Vespa</span><span class="c0">&nbsp;servira de plateforme de recherche IA haute performance et de base de donn&eacute;es vectorielle. Son r&ocirc;le est d&#39;indexer efficacement les plongements de documents g&eacute;n&eacute;r&eacute;s et d&#39;ex&eacute;cuter des requ&ecirc;tes de recherche neuronale sophistiqu&eacute;es &agrave; grande &eacute;chelle.</span><span class="c13 c0">7</span></li><li class="c4 c12 li-bullet-0"><span class="c8">ColBERT (Interaction Tardive)</span><span class="c0">&nbsp;sera int&eacute;gr&eacute; aux capacit&eacute;s de recherche de Vespa. La force de ColBERT r&eacute;side dans sa correspondance s&eacute;mantique fine au niveau des jetons, ce qui am&eacute;liorera consid&eacute;rablement la pertinence et la pr&eacute;cision des documents r&eacute;cup&eacute;r&eacute;s.</span><span class="c13 c0">9</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Ollama</span><span class="c0">&nbsp;sera utilis&eacute; pour h&eacute;berger un grand mod&egrave;le de langage (LLM) sp&eacute;cifiquement optimis&eacute; pour les environnements CPU uniquement. Cela permettra une g&eacute;n&eacute;ration de texte locale, priv&eacute;e et rentable, en tirant parti du contexte r&eacute;cup&eacute;r&eacute; par Vespa.</span><span class="c13 c0">11</span></li><li class="c4 c12 li-bullet-0"><span class="c0">L&#39;ensemble du syst&egrave;me sera d&eacute;ploy&eacute; et g&eacute;r&eacute; &agrave; l&#39;aide de </span><span class="c8">Docker Compose</span><span class="c0">. Cet outil d&#39;orchestration assurera une communication inter-services transparente, simplifiera le processus de configuration et facilitera la gestion continue des composants int&eacute;gr&eacute;s.</span><span class="c13 c0">13</span></li></ul><p class="c4"><span class="c0">L&#39;int&eacute;gration de la recherche neuronale et de la RAG pour la gestion de documents est essentielle. Alors que Paperless-ngx excelle dans l&#39;organisation de base des documents, son syst&egrave;me de recherche natif, bas&eacute; sur le texte int&eacute;gral et la correspondance de m&eacute;tadonn&eacute;es simples, ne peut pas saisir les nuances s&eacute;mantiques requises pour des requ&ecirc;tes complexes. La recherche neuronale, en particulier avec ColBERT, apporte cette profondeur s&eacute;mantique, permettant une compr&eacute;hension contextuelle qui va au-del&agrave; des correspondances exactes de mots. Le r&ocirc;le de la RAG est alors de donner un but &agrave; cette r&eacute;cup&eacute;ration s&eacute;mantique : fournir des r&eacute;ponses pr&eacute;cises et v&eacute;rifiables via un LLM. Cette combinaison transforme une archive statique en une base de connaissances dynamique et intelligente, am&eacute;liorant consid&eacute;rablement la productivit&eacute; et r&eacute;duisant les efforts manuels de r&eacute;cup&eacute;ration d&#39;informations. Le syst&egrave;me agit comme un assistant proactif, capable de comprendre l&#39;intention de l&#39;utilisateur et le contexte de sa demande.</span><span class="c13 c0">2</span></p><p class="c4"><span class="c7 c0">La contrainte de fonctionner sans GPU pour le LLM est un facteur d&eacute;terminant pour la conception de l&#39;ensemble du syst&egrave;me. Elle a des implications profondes sur le choix des mod&egrave;les (n&eacute;cessitant des LLM plus petits et fortement quantifi&eacute;s), sur la vitesse d&#39;inf&eacute;rence attendue du LLM (plus lente qu&#39;avec un GPU), et sur la n&eacute;cessit&eacute; d&#39;une gestion efficace des ressources pour les plongements ColBERT au sein de Vespa. Cette exigence n&#39;est pas un obstacle mineur, mais un moteur de conception principal, qui oriente l&#39;attention vers des composants d&#39;IA hautement optimis&eacute;s et &eacute;conomes en ressources. Chaque d&eacute;cision, de la quantification du LLM aux strat&eacute;gies d&#39;indexation de Vespa pour ColBERT, doit donner la priorit&eacute; &agrave; la minimisation de la charge de calcul afin de garantir une latence et une convivialit&eacute; acceptables. Cette approche met en lumi&egrave;re une tendance croissante dans l&#39;IA : la recherche d&#39;une IA accessible, locale et respectueuse de la vie priv&eacute;e. Le sc&eacute;nario &quot;sans GPU&quot; repousse les limites de ce qui est r&eacute;alisable sur du mat&eacute;riel grand public, d&eacute;mocratisant ainsi l&#39;IA avanc&eacute;e, mais exigeant une ing&eacute;nierie m&eacute;ticuleuse. Le succ&egrave;s de cette solution auto-h&eacute;berg&eacute;e d&eacute;pend directement de cette optimisation multicouche.</span></p><p class="c4 c10"><span class="c7 c0"></span></p><h2 class="c17"><span class="c26 c9">2. Plong&eacute;e Profonde dans les Technologies Cl&eacute;s</span></h2><p class="c4 c10"><span class="c7 c21"></span></p><p class="c4 c10"><span class="c7 c21"></span></p><h3 class="c22"><span class="c6">ColBERT : Interaction Tardive Contextualis&eacute;e pour une R&eacute;cup&eacute;ration Fine</span></h3><p class="c4 c10"><span class="c7 c21"></span></p><p class="c4"><span class="c0">ColBERT (Contextualized Late Interaction over BERT) se distingue des approches bi-encodeurs standard en g&eacute;n&eacute;rant une </span><span class="c0 c20">repr&eacute;sentation matricielle (multivectorielle)</span><span class="c0">&nbsp;du texte d&#39;entr&eacute;e, plut&ocirc;t qu&#39;un seul vecteur de dimension fixe pour l&#39;ensemble de la requ&ecirc;te ou du document. Cela signifie qu&#39;il produit un plongement vectoriel distinct pour chaque jeton (ou sous-mot) dans le texte d&#39;entr&eacute;e. Cette approche de &quot;sac de vecteurs&quot; permet &agrave; ColBERT de capturer des s&eacute;mantiques d&#39;entr&eacute;e plus nuanc&eacute;es et des relations s&eacute;mantiques plus fines par rapport aux mod&egrave;les qui compressent une entr&eacute;e enti&egrave;re en un seul vecteur.</span><span class="c13 c0">9</span></p><p class="c4"><span class="c0">Le processus implique la tokenisation de la requ&ecirc;te et du document (par exemple, &agrave; l&#39;aide du tokenizer WordPiece de BERT). Chaque jeton, ainsi que son contexte environnant, est ensuite trait&eacute; via une architecture d&#39;encodeur de transformateur modifi&eacute;e (telle que XLM-RoBERTa, comme on le voit dans Jina-ColBERT-v2). Cela produit un plongement contextualis&eacute; pour chaque jeton. Pour maintenir la convivialit&eacute; pratique et la compatibilit&eacute; avec l&#39;infrastructure existante de similarit&eacute; vectorielle, la dimensionnalit&eacute; de sortie de ces plongements de jetons est g&eacute;n&eacute;ralement contrainte (par exemple, r&eacute;duite de 128 &agrave; 64 dimensions dans ColBERTv2, avec un compromis de performance minimal). Malgr&eacute; cela, le stockage de plusieurs vecteurs par document n&eacute;cessite intrins&egrave;quement plus d&#39;espace qu&#39;une repr&eacute;sentation &agrave; vecteur unique.</span><span class="c13 c0">9</span></p><p class="c4"><span class="c0">Le trait distinctif de ColBERT est son m&eacute;canisme d&#39;&quot;interaction tardive&quot;. Contrairement aux cross-encodeurs, qui concat&egrave;nent une requ&ecirc;te et un document en une seule entr&eacute;e pour un traitement conjoint (&quot;interaction pr&eacute;coce&quot;), ColBERT encode les requ&ecirc;tes et les documents </span><span class="c0 c20">ind&eacute;pendamment</span><span class="c0">. L&#39;interaction cruciale, ou comparaison, se produit </span><span class="c0 c20">plus tard</span><span class="c0">, pendant la phase de notation.</span><span class="c14 c0">9</span><span class="c7 c0">&nbsp;Le score de pertinence entre une requ&ecirc;te et un document est calcul&eacute; en fonction des similarit&eacute;s entre leurs plongements de jetons respectifs. Plus pr&eacute;cis&eacute;ment, pour chaque jeton de la requ&ecirc;te, sa similarit&eacute; (g&eacute;n&eacute;ralement en utilisant le produit scalaire) est calcul&eacute;e par rapport &agrave;</span></p><p class="c4"><span class="c0 c20">tous</span><span class="c0">&nbsp;les plongements de jetons du document. La similarit&eacute; maximale (MaxSim) trouv&eacute;e pour chaque jeton de requ&ecirc;te individuel est ensuite identifi&eacute;e. Le score de pertinence global pour le document est la </span><span class="c0 c20">somme de ces valeurs MaxSim</span><span class="c0">&nbsp;sur tous les jetons de la requ&ecirc;te.</span><span class="c14 c0">15</span><span class="c0">&nbsp;Cette m&eacute;thode de score par interaction tardive approche efficacement l&#39;attention conjointe requ&ecirc;te-document observ&eacute;e dans les cross-encodeurs plus co&ucirc;teux en calcul, tout en maintenant une efficacit&eacute; d&#39;inf&eacute;rence plus proche de celle des mod&egrave;les de r&eacute;cup&eacute;ration dense traditionnels. Cet &eacute;quilibre rend ColBERT adapt&eacute; aux applications &agrave; grande &eacute;chelle n&eacute;cessitant une grande pr&eacute;cision sans co&ucirc;ts de calcul prohibitifs.</span><span class="c13 c0">9</span></p><p class="c4"><span class="c7 c0">Les plongements multivectoriels g&eacute;n&eacute;r&eacute;s par ColBERT sont parfaitement compatibles avec le support multivectoriel de Vespa, ce qui constitue un levier de performance essentiel. L&#39;int&eacute;gration native permet d&#39;indexer les plongements au niveau des jetons directement dans Vespa. Cela signifie que le calcul intensif de MaxSim, qui est au c&oelig;ur de la notation de pertinence fine de ColBERT, peut &ecirc;tre effectu&eacute; efficacement au sein du moteur de classement hautement optimis&eacute; et distribu&eacute; de Vespa. Cette approche minimise les frais g&eacute;n&eacute;raux de transfert de donn&eacute;es et tire parti des capacit&eacute;s de traitement parall&egrave;le de Vespa, rendant la pr&eacute;cision de ColBERT &eacute;volutive et performante dans un environnement de production. L&#39;efficacit&eacute; des mod&egrave;les de r&eacute;cup&eacute;ration neuronale avanc&eacute;s comme ColBERT dans un syst&egrave;me &agrave; grande &eacute;chelle d&eacute;pend fortement de la capacit&eacute; du moteur de recherche sous-jacent &agrave; prendre en charge nativement et &agrave; traiter efficacement leurs structures d&#39;int&eacute;gration uniques et leurs m&eacute;canismes d&#39;interaction. Cette int&eacute;gration est un facteur cl&eacute; pour obtenir une RAG de haute qualit&eacute;.</span></p><p class="c4"><span class="c0">Les avantages de ColBERT et ses cas d&#39;utilisation, notamment le reranking dans les syst&egrave;mes RAG, sont multiples. Sa correspondance fine, gr&acirc;ce &agrave; la comparaison jeton par jeton et &agrave; l&#39;agr&eacute;gation MaxSim, permet &agrave; ColBERT de se concentrer sur les parties les plus pertinentes d&#39;un document, ce qui conduit &agrave; une pr&eacute;cision et une robustesse sup&eacute;rieures, en particulier pour les requ&ecirc;tes nuanc&eacute;es ou hors domaine.</span><span class="c14 c0">9</span><span class="c0">&nbsp;Un avantage significatif est la capacit&eacute; de pr&eacute;calculer et d&#39;indexer les plongements de documents, ce qui rend la r&eacute;cup&eacute;ration beaucoup plus rapide que les cross-encodeurs. Par cons&eacute;quent, ColBERT est souvent recommand&eacute; pour le reranking d&#39;un ensemble plus petit de documents candidats (par exemple, 100 &agrave; 500) initialement r&eacute;cup&eacute;r&eacute;s par un r&eacute;cup&eacute;rateur dense plus simple et plus rapide, pla&ccedil;ant ainsi les r&eacute;sultats les plus pertinents en t&ecirc;te.</span><span class="c14 c0">10</span><span class="c0">&nbsp;La conscience du contexte, inh&eacute;rente &agrave; la conception de ColBERT, lui permet de saisir la signification nuanc&eacute;e des mots, m&ecirc;me lorsqu&#39;ils apparaissent dans des contextes diff&eacute;rents.</span><span class="c14 c0">17</span><span class="c0">&nbsp;Dans les syst&egrave;mes RAG, la pr&eacute;cision et l&#39;efficacit&eacute; de ColBERT le rendent exceptionnellement efficace en tant que reranker, car une r&eacute;cup&eacute;ration pr&eacute;cise et contextuellement riche des documents a un impact direct sur la qualit&eacute; et l&#39;exactitude factuelle des r&eacute;ponses g&eacute;n&eacute;r&eacute;es par le LLM.</span><span class="c14 c0">17</span><span class="c0">&nbsp;Bien que ColBERTv2 soit monolingue, des extensions multilingues comme ColBERT-XM ont &eacute;t&eacute; d&eacute;velopp&eacute;es pour r&eacute;pondre aux besoins de r&eacute;cup&eacute;ration multilingue.</span><span class="c13 c0">9</span></p><p class="c4"><span class="c7 c0">Le tableau suivant compare les architectures de mod&egrave;les de r&eacute;cup&eacute;ration neuronale :</span></p><p class="c4"><span class="c3">Tableau 1 : Comparaison des Architectures de Mod&egrave;les de R&eacute;cup&eacute;ration Neurale</span></p><table class="c25"><tr class="c24"><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Caract&eacute;ristique / Type de Mod&egrave;le</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Bi-encodeur (ex: Sentence-BERT)</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Cross-encodeur (ex: BERT-base)</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">ColBERT (Interaction Tardive Contextualis&eacute;e)</span></p></td></tr><tr class="c24"><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c3">M&eacute;canisme d&#39;Interaction</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Encodage ind&eacute;pendant, similarit&eacute; pr&eacute;coce</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Encodage conjoint, interaction pr&eacute;coce</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Encodage ind&eacute;pendant, interaction tardive</span></p></td></tr><tr class="c24"><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c3">Encodage</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Requ&ecirc;te et document encod&eacute;s s&eacute;par&eacute;ment en un seul vecteur chacun</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Requ&ecirc;te et document concat&eacute;n&eacute;s et encod&eacute;s ensemble en une seule s&eacute;quence</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c0">Requ&ecirc;te et document encod&eacute;s s&eacute;par&eacute;ment en </span><span class="c0 c20">plusieurs</span><span class="c7 c0">&nbsp;vecteurs (un par jeton)</span></p></td></tr><tr class="c24"><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c3">Calcul de Similarit&eacute;</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Produit scalaire ou distance cosinus entre les vecteurs uniques</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Attention conjointe au sein du mod&egrave;le, score de pertinence final</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">MaxSim (similarit&eacute; maximale par jeton de requ&ecirc;te, puis somme)</span></p></td></tr><tr class="c24"><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c3">Efficacit&eacute; d&#39;Inf&eacute;rence</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Tr&egrave;s rapide (plongements de document pr&eacute;calcul&eacute;s)</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Tr&egrave;s lent (n&eacute;cessite un nouveau calcul pour chaque paire requ&ecirc;te-document)</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Rapide (plongements de document pr&eacute;calcul&eacute;s, interaction tardive efficace)</span></p></td></tr><tr class="c24"><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c3">Pr&eacute;cision</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Moins pr&eacute;cise pour les requ&ecirc;tes fines (perte de contexte)</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Tr&egrave;s pr&eacute;cise (capture les interactions fines)</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Tr&egrave;s pr&eacute;cise (approche les cross-encodeurs avec une meilleure efficacit&eacute;)</span></p></td></tr><tr class="c24"><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c3">Co&ucirc;t de Calcul</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Faible</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">&Eacute;lev&eacute;</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Mod&eacute;r&eacute; (compromis &eacute;quilibr&eacute;)</span></p></td></tr><tr class="c24"><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c3">Cas d&#39;Usage Typique</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">R&eacute;cup&eacute;ration initiale &agrave; grande &eacute;chelle, filtrage</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Re-classement de pr&eacute;cision sur un petit ensemble de candidats</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c4"><span class="c7 c0">Re-classement dans les pipelines RAG, recherche s&eacute;mantique fine</span></p></td></tr></table><p class="c10 c28"><span class="c7 c0"></span></p><h3 class="c22"><span class="c6">Vespa : La Plateforme de Recherche IA et de Base de Donn&eacute;es Vectorielle</span></h3><p class="c4 c10"><span class="c7 c21"></span></p><p class="c4"><span class="c0">Vespa est une plateforme avanc&eacute;e con&ccedil;ue pour les applications n&eacute;cessitant un calcul &agrave; faible latence sur des ensembles de donn&eacute;es massifs. Elle peut g&eacute;rer des syst&egrave;mes allant jusqu&#39;&agrave; des centaines de n&oelig;uds, g&eacute;rant des dizaines de milliards de documents et des milliers de requ&ecirc;tes par seconde, avec des temps de r&eacute;ponse typiques de l&#39;ordre de quelques dizaines de millisecondes. Son architecture garantit des temps de r&eacute;ponse constants quel que soit le volume de donn&eacute;es en ex&eacute;cutant les requ&ecirc;tes en parall&egrave;le sur de nombreux shards de donn&eacute;es et c&oelig;urs.</span><span class="c13 c0">7</span></p><p class="c4"><span class="c0">Une capacit&eacute; fondamentale de Vespa est son support natif des tenseurs, qui facilite le classement et la prise de d&eacute;cision complexes. Vespa int&egrave;gre l&#39;inf&eacute;rence de mod&egrave;les appris par machine distribu&eacute;s pour la notation de pertinence, ce qui en fait un choix id&eacute;al pour les applications d&#39;IA en temps r&eacute;el telles que la RAG, les moteurs de recommandation et la recherche intelligente &agrave; l&#39;&eacute;chelle de l&#39;entreprise.</span><span class="c14 c0">7</span><span class="c0">&nbsp;Vespa est con&ccedil;u pour les changements de donn&eacute;es continus, supportant des volumes d&#39;&eacute;criture &eacute;lev&eacute;s (des milliers &agrave; des dizaines de milliers par n&oelig;ud et par seconde) simultan&eacute;ment avec le service de requ&ecirc;tes. Il maintient automatiquement une distribution &eacute;quilibr&eacute;e des donn&eacute;es et une redondance configurable, m&ecirc;me lorsque des n&oelig;uds sont ajout&eacute;s, supprim&eacute;s ou perdus de mani&egrave;re inattendue.</span><span class="c13 c0">7</span></p><p class="c4"><span class="c0">Vespa offre de solides capacit&eacute;s d&#39;indexation multivectorielle, permettant aux applications de stocker et de g&eacute;rer plusieurs vecteurs par document. Cette fonctionnalit&eacute; est cruciale pour l&#39;int&eacute;gration de ColBERT, qui produit des plongements au niveau des jetons, car elle permet de r&eacute;cup&eacute;rer des documents en fonction du vecteur le plus proche dans un document par rapport au vecteur de la requ&ecirc;te.</span><span class="c14 c0">7</span><span class="c0">&nbsp;Il fournit une fonctionnalit&eacute; de recherche de voisins les plus proches approximatifs (ANN) via un algorithme de graphe Hierarchical Navigable Small World (HNSW hautement optimis&eacute;). Cette m&eacute;thode est nettement plus rentable pour les grands ensembles de donn&eacute;es que la recherche exacte (force brute). L&#39;impl&eacute;mentation HNSW de Vespa prend en charge le filtrage, l&#39;indexation multivectorielle et les op&eacute;rations CRUD en temps r&eacute;el sur les vecteurs, garantissant des index dynamiques et &agrave; jour.</span><span class="c14 c0">7</span><span class="c7 c0">&nbsp;L&#39;op&eacute;rateur de requ&ecirc;te</span></p><p class="c4"><span class="c0">nearestNeighbor est central pour la recherche vectorielle dans Vespa et peut &ecirc;tre combin&eacute; de mani&egrave;re transparente avec des filtres structur&eacute;s et des op&eacute;rateurs de recherche de texte traditionnels &agrave; l&#39;aide du langage de requ&ecirc;te Vespa (YQL). Cela permet la cr&eacute;ation de solutions de r&eacute;cup&eacute;ration hybrides puissantes qui exploitent &agrave; la fois les signaux s&eacute;mantiques et lexicaux.</span><span class="c14 c0">7</span><span class="c7 c0">&nbsp;Vespa prend en charge une vari&eacute;t&eacute; de m&eacute;triques de distance (par exemple,</span></p><p class="c4"><span class="c0">euclidean, angular, dotproduct, prenormalized-angular, hamming, geodegrees), qui sont configur&eacute;es au niveau du champ tenseur et utilis&eacute;es &agrave; la fois pour la construction de l&#39;index et le calcul de la distance au moment de la requ&ecirc;te.</span><span class="c13 c0">7</span></p><p class="c4"><span class="c0">Le classement dans Vespa est configur&eacute; via des expressions de classement d&eacute;claratives d&eacute;finies dans des &quot;profils de classement&quot; dans les sch&eacute;mas de documents. Ces expressions peuvent aller d&#39;op&eacute;rations math&eacute;matiques de base utilisant des fonctionnalit&eacute;s de classement int&eacute;gr&eacute;es &agrave; des expressions tensorielles tr&egrave;s complexes et des mod&egrave;les appris par machine int&eacute;gr&eacute;s (prenant en charge des formats tels que ONNX, XGBoost et LightGBM).</span><span class="c14 c0">7</span><span class="c7 c0">&nbsp;La capacit&eacute; de classement par phases de Vespa est une optimisation puissante. Elle permet plusieurs &eacute;tapes de classement : une expression</span></p><p class="c4"><span class="c0">first-phase peu co&ucirc;teuse est appliqu&eacute;e &agrave; tous les documents r&eacute;cup&eacute;r&eacute;s, suivie d&#39;une second-phase plus intensive en calcul (et &eacute;ventuellement une global-phase) ex&eacute;cut&eacute;e uniquement sur les documents les mieux class&eacute;s de la phase pr&eacute;c&eacute;dente. Cette strat&eacute;gie dirige efficacement plus de calcul vers les documents candidats les plus prometteurs, optimisant consid&eacute;rablement les performances globales des requ&ecirc;tes.</span><span class="c14 c0">7</span><span class="c7 c0">&nbsp;Pour impl&eacute;menter l&#39;interaction tardive de ColBERT, la capacit&eacute; de Vespa &agrave; d&eacute;finir des fonctions de classement personnalis&eacute;es &agrave; l&#39;aide de tenseurs est essentielle. La fonction de classement</span></p><p class="c4"><span class="c0">closeness() peut &ecirc;tre utilis&eacute;e avec des champs multivectoriels pour trouver le vecteur le plus proche, ce qui est un composant cl&eacute; pour MaxSim. Vespa prend &eacute;galement en charge unpack_bits pour g&eacute;rer les vecteurs binaires compress&eacute;s, ce qui peut &ecirc;tre particuli&egrave;rement utile pour le stockage &eacute;conome en m&eacute;moire des vecteurs de jetons de ColBERT.</span><span class="c13 c0">7</span></p><p class="c4 c10"><span class="c13 c0"></span></p><h3 class="c22"><span class="c6">Paperless-ngx AI : Gestion Intelligente des Documents</span></h3><p class="c4 c10"><span class="c7 c21"></span></p><p class="c4"><span class="c0">Paperless-ngx est un syst&egrave;me de gestion de documents open source con&ccedil;u pour transformer les documents physiques et num&eacute;riques en une archive consultable et organis&eacute;e. Il effectue la reconnaissance optique de caract&egrave;res (OCR) sur les documents sans texte int&eacute;gr&eacute; &agrave; l&#39;aide du moteur open source Tesseract, qui prend en charge plus de 100 langues. Les documents sont stock&eacute;s au format PDF/A pour l&#39;archivage &agrave; long terme, aux c&ocirc;t&eacute;s de leurs originaux non alt&eacute;r&eacute;s.</span><span class="c13 c0">6</span></p><p class="c4"><span class="c0">Une caract&eacute;ristique intelligente essentielle est son utilisation de l&#39;apprentissage automatique pour attribuer automatiquement des m&eacute;tadonn&eacute;es telles que des &eacute;tiquettes, des correspondants, des types de documents et des chemins de stockage. Les utilisateurs peuvent configurer divers algorithmes de correspondance (par exemple, &quot;Any&quot;, &quot;All&quot;, &quot;Exact&quot;, &quot;Regular expression&quot;, &quot;Fuzzy match&quot;, ou &quot;Auto&quot;) ou utiliser un mode manuel avec assistance IA pour la r&eacute;vision.</span><span class="c14 c0">5</span><span class="c0">&nbsp;Les fonctionnalit&eacute;s cl&eacute;s incluent une puissante recherche en texte int&eacute;gral avec auto-compl&eacute;tion et surlignage, une fonction &quot;Plus comme &ccedil;a&quot; pour trouver des documents similaires, de solides capacit&eacute;s de traitement des e-mails, un syst&egrave;me de permissions multi-utilisateurs robuste et des flux de travail personnalisables pour automatiser la gestion des documents.</span><span class="c13 c0">6</span></p><p class="c4"><span class="c0">L&#39;extension &quot;Paperless-AI&quot; augmente consid&eacute;rablement les capacit&eacute;s de Paperless-ngx en int&eacute;grant des fonctions d&#39;IA avanc&eacute;es pour l&#39;analyse automatis&eacute;e des documents et m&ecirc;me une fonction de chat interactive permettant de poser des questions directement sur les documents.</span><span class="c14 c0">5</span><span class="c7 c0">&nbsp;Un avantage crucial pour les utilisateurs soucieux de la confidentialit&eacute; et les sc&eacute;narios d&#39;auto-h&eacute;bergement est la flexibilit&eacute; de choisir les backends d&#39;IA. Cela inclut la possibilit&eacute; d&#39;utiliser des services bas&eacute;s sur le cloud (comme OpenAI ou Azure) ou, de mani&egrave;re cruciale, de tirer parti des</span></p><p class="c4"><span class="c0 c20">mod&egrave;les locaux via Ollama</span><span class="c0">.</span><span class="c14 c0">5</span><span class="c0">&nbsp;La politique de confidentialit&eacute; du projet Paperless-AI stipule explicitement qu&#39;il agit comme une interface, envoyant le contenu des documents crypt&eacute; uniquement au serveur Paperless-AI configur&eacute;, sans aucune donn&eacute;e transmise aux d&eacute;veloppeurs ou &agrave; des tiers, renfor&ccedil;ant ainsi le contr&ocirc;le local des donn&eacute;es.</span><span class="c13 c0">5</span></p><p class="c4"><span class="c0">Paperless-ngx expose une API REST compl&egrave;te, offrant diverses m&eacute;thodes d&#39;authentification (Basic, Session, Token et Remote User), permettant une interaction programmatique avec le syst&egrave;me.</span><span class="c14 c0">27</span><span class="c7 c0">&nbsp;L&#39;API prend en charge les requ&ecirc;tes de recherche en texte int&eacute;gral (</span></p><p class="c4"><span class="c0">/api/documents/?query=...) et les recherches &quot;plus comme &ccedil;a&quot; (/api/documents/?more_like_id=...), renvoyant des r&eacute;sultats avec un score de pertinence, des highlights des termes correspondants et un rank.</span><span class="c0 c14">27</span><span class="c0">&nbsp;Au-del&agrave; de la recherche, l&#39;API fournit des m&eacute;thodes pour g&eacute;rer les documents et leurs m&eacute;tadonn&eacute;es, y compris la d&eacute;finition des correspondants, des types de documents, des chemins de stockage, l&#39;ajout/la suppression d&#39;&eacute;tiquettes, la suppression, le retraitement, la fusion, le fractionnement, la rotation des pages et la modification des champs personnalis&eacute;s.</span><span class="c14 c0">27</span><span class="c0">&nbsp;Les documents peuvent &ecirc;tre t&eacute;l&eacute;charg&eacute;s via l&#39;interface utilisateur, et le syst&egrave;me stocke syst&eacute;matiquement les documents originaux tout en cr&eacute;ant &eacute;galement des versions PDF/A archivables.</span><span class="c14 c0">26</span><span class="c7 c0">&nbsp;Pour la gestion des donn&eacute;es en vrac, un outil</span></p><p class="c4"><span class="c0">document_exporter est disponible, capable d&#39;exporter tous les documents, vignettes, m&eacute;tadonn&eacute;es et contenus de base de donn&eacute;es vers un dossier sp&eacute;cifi&eacute;. Cette fonctionnalit&eacute; est essentielle pour les sauvegardes incr&eacute;mentielles ou la migration de documents vers d&#39;autres syst&egrave;mes de gestion de documents (DMS).</span><span class="c13 c0">29</span></p><p class="c4"><span class="c7 c0">L&#39;int&eacute;gration des m&eacute;tadonn&eacute;es de Paperless-ngx et de la recherche s&eacute;mantique de ColBERT cr&eacute;e une approche de r&eacute;cup&eacute;ration hybride puissante. Paperless-ngx excelle &agrave; fournir des attributs structur&eacute;s pour les documents (tags, correspondants, types de documents). ColBERT, quant &agrave; lui, offre une compr&eacute;hension s&eacute;mantique approfondie. Lorsque ces capacit&eacute;s sont combin&eacute;es, les m&eacute;tadonn&eacute;es de Paperless-ngx peuvent servir de filtres tr&egrave;s efficaces dans les requ&ecirc;tes de Vespa, r&eacute;duisant rapidement l&#39;espace de recherche &agrave; un sous-ensemble pertinent de documents. Apr&egrave;s ce filtrage initial par m&eacute;tadonn&eacute;es structur&eacute;es, la recherche s&eacute;mantique de ColBERT peut effectuer son re-classement fin au niveau des jetons sur cet ensemble de candidats plus petit et plus cibl&eacute;. Cela cr&eacute;e un pipeline de r&eacute;cup&eacute;ration hybride multi-&eacute;tapes qui tire parti de la pr&eacute;cision des donn&eacute;es structur&eacute;es pour la r&eacute;cup&eacute;ration initiale et de la flexibilit&eacute; de la compr&eacute;hension s&eacute;mantique pour un classement nuanc&eacute;. Il en r&eacute;sulte une exp&eacute;rience de recherche plus robuste et conviviale. Les utilisateurs peuvent exploiter simultan&eacute;ment les attributs explicites des documents (par exemple, &quot;afficher les factures du fournisseur X&quot;) et le contenu s&eacute;mantique implicite (par exemple, &quot;trouver des documents sur la restructuration financi&egrave;re&quot;), ce qui conduit &agrave; des r&eacute;ponses RAG plus pr&eacute;cises et plus efficaces. C&#39;est une approche qui combine le meilleur des deux mondes pour la r&eacute;cup&eacute;ration d&#39;informations.</span></p><p class="c4 c10"><span class="c7 c0"></span></p><h3 class="c22"><span class="c6">Ollama : Inf&eacute;rence LLM Locale pour les Environnements CPU Uniquement</span></h3><p class="c4 c10"><span class="c7 c21"></span></p><p class="c4"><span class="c0">Ollama (Open LLM Application Manager) est une plateforme open source sp&eacute;cifiquement con&ccedil;ue pour d&eacute;ployer et interagir avec des grands mod&egrave;les de langage (LLM) open source localement, prenant en charge l&#39;inf&eacute;rence CPU et GPU.</span><span class="c14 c0">11</span><span class="c0">&nbsp;Pour l&#39;inf&eacute;rence CPU uniquement, Ollama peut offrir des performances acceptables pour certains cas d&#39;utilisation, tels que les op&eacute;rations par lots importantes ou l&#39;acc&egrave;s programmatique o&ugrave; des r&eacute;ponses imm&eacute;diates et en temps r&eacute;el ne sont pas strictement requises. Cependant, elle sera nettement plus lente que les solutions bas&eacute;es sur GPU, avec des retards potentiels de 30 &agrave; 60 secondes pour les op&eacute;rations par lots, contre 2 &agrave; 10 secondes sur GPU. Pour les questions interactives complexes, il peut y avoir un d&eacute;lai de 5 &agrave; 10 secondes avant la r&eacute;ponse, et la g&eacute;n&eacute;ration de texte peut sembler lente, rappelant les vitesses des modems plus anciens.</span><span class="c14 c0">12</span><span class="c0">&nbsp;Les exigences mat&eacute;rielles pour l&#39;inf&eacute;rence CPU uniquement sont substantielles, n&eacute;cessitant g&eacute;n&eacute;ralement un CPU relativement puissant (par exemple, un &eacute;quivalent Intel i7) et des quantit&eacute;s importantes de RAM. Les mod&egrave;les plus petits (par exemple, 7 milliards de param&egrave;tres) peuvent n&eacute;cessiter 16 &agrave; 32 Go de RAM, tandis qu&#39;un mod&egrave;le de 14 milliards de param&egrave;tres pourrait n&eacute;cessiter environ 20 Go de RAM.</span><span class="c14 c0">12</span><span class="c0">&nbsp;Les performances attendues sur une configuration CPU uniquement avec des mod&egrave;les quantifi&eacute;s comme Llama 2 ou Mistral-7B se situent g&eacute;n&eacute;ralement dans la plage de 5 &agrave; 10 jetons par seconde.</span><span class="c13 c0">33</span></p><p class="c4"><span class="c0">La quantification est une strat&eacute;gie essentielle et fr&eacute;quemment appliqu&eacute;e pour rendre les mod&egrave;les d&#39;apprentissage automatique volumineux et complexes, en particulier les LLM, suffisamment l&eacute;gers pour fonctionner dans des environnements contraints en ressources comme les machines locales sans GPU d&eacute;di&eacute;s. Elle implique la r&eacute;duction de la pr&eacute;cision num&eacute;rique des param&egrave;tres (poids) du mod&egrave;le, g&eacute;n&eacute;ralement de la virgule flottante 32 bits &agrave; des repr&eacute;sentations inf&eacute;rieures comme les entiers 8 bits ou 4 bits. Cela r&eacute;duit consid&eacute;rablement l&#39;empreinte m&eacute;moire et acc&eacute;l&egrave;re la vitesse d&#39;inf&eacute;rence.</span><span class="c14 c0">30</span><span class="c0">&nbsp;Ollama prend en charge de mani&egrave;re transparente un large &eacute;ventail de mod&egrave;les quantifi&eacute;s, souvent au format GGUF, qui est sp&eacute;cifiquement optimis&eacute; pour l&#39;inf&eacute;rence sur machine locale.</span><span class="c13 c0">30</span></p><p class="c4"><span class="c7 c0">Les LLM optimis&eacute;s pour le CPU recommand&eacute;s (souvent disponibles en versions quantifi&eacute;es 4 bits ou 8 bits pour l&#39;efficacit&eacute;) incluent :</span></p><ul class="c1 lst-kix_list_2-0 start"><li class="c4 c12 li-bullet-0"><span class="c8">Mistral 7B (versions quantifi&eacute;es) :</span><span class="c0">&nbsp;Lou&eacute; pour son excellent rapport performance/taille et ses capacit&eacute;s de raisonnement d&eacute;centes, ce qui en fait un candidat solide pour les configurations CPU uniquement.</span><span class="c13 c0">12</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Llama 2 (7B ou 13B, quantifi&eacute;) :</span><span class="c0">&nbsp;Un mod&egrave;le largement pris en charge et capable pour les t&acirc;ches g&eacute;n&eacute;rales de traitement du langage naturel. Les versions quantifi&eacute;es sont essentielles pour s&#39;adapter aux limites de RAM CPU typiques et optimiser les performances.</span><span class="c13 c0">30</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Gemma 7B (quantifi&eacute;) :</span><span class="c0">&nbsp;En tant que grand fr&egrave;re de la famille Gemma, il offre des capacit&eacute;s am&eacute;lior&eacute;es et partage des composants architecturaux avec les mod&egrave;les Gemini plus grands de Google, permettant des performances &eacute;lev&eacute;es sur du mat&eacute;riel grand public. Les versions quantifi&eacute;es (par exemple, Q5_K_M avec une taille de fichier de 6,14 Go) peuvent tenir confortablement dans 8 Go de RAM syst&egrave;me pour des performances optimales.</span><span class="c13 c0">30</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Phi-3 Mini (3.8B, quantifi&eacute;) :</span><span class="c0">&nbsp;Le mod&egrave;le l&eacute;ger et de pointe de Microsoft, distingu&eacute; par une efficacit&eacute; exceptionnelle et une forte concentration sur des propri&eacute;t&eacute;s de haute qualit&eacute; et de raisonnement dense. Il est remarquablement &eacute;conome en m&eacute;moire (par exemple, la quantification Q8_0 est un fichier de 4,06 Go, n&eacute;cessitant environ 7,48 Go de m&eacute;moire), ce qui le rend adapt&eacute; aux environnements contraints en m&eacute;moire/calcul et aux sc&eacute;narios &agrave; latence limit&eacute;e.</span><span class="c13 c0">35</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Hermes 3 Llama 3.2 3B/8B (quantifi&eacute;) :</span><span class="c0">&nbsp;Not&eacute; pour son ob&eacute;issance et ses solides performances, en particulier lorsqu&#39;il utilise des balises d&#39;appel de fonction.</span><span class="c13 c0">33</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Mod&egrave;les TinyLLaMA :</span><span class="c0">&nbsp;Impressionnants malgr&eacute; leur tr&egrave;s petite empreinte, ce qui les rend tr&egrave;s accessibles pour les configurations minimales.</span><span class="c13 c0">32</span></li></ul><p class="c4"><span class="c0">Pour les syst&egrave;mes CPU uniquement, une quantification agressive (par exemple, 4 bits) est fortement recommand&eacute;e, car elle donne souvent de meilleures performances globales que les mod&egrave;les l&eacute;g&egrave;rement plus grands et moins agressivement quantifi&eacute;s.</span><span class="c13 c0">32</span></p><p class="c4"><span class="c7 c0">La quantification des mod&egrave;les par Ollama constitue un pont vers une IA locale omnipr&eacute;sente. La capacit&eacute; d&#39;Ollama &agrave; ex&eacute;cuter efficacement des LLM quantifi&eacute;s sur du mat&eacute;riel CPU uniquement est un facteur fondamental pour r&eacute;pondre &agrave; l&#39;exigence d&#39;un syst&egrave;me &quot;auto-h&eacute;berg&eacute;&quot; et &quot;sans GPU&quot;. Cette capacit&eacute; rend l&#39;IA g&eacute;n&eacute;rative puissante accessible sans les co&ucirc;ts prohibitifs ou la d&eacute;pendance au cloud des solutions acc&eacute;l&eacute;r&eacute;es par GPU. La quantification &quot;d&eacute;mocratise&quot; l&#39;acc&egrave;s aux LLM avanc&eacute;s, d&eacute;pla&ccedil;ant l&#39;accent de la puissance de calcul brute vers la compression intelligente des mod&egrave;les et des piles logicielles efficaces. Cela transforme les LLM de technologies exclusives aux GPU en outils viables pour les d&eacute;ploiements locaux, personnels ou &agrave; petite &eacute;chelle en entreprise. Cela signifie que le syst&egrave;me peut &ecirc;tre enti&egrave;rement priv&eacute; et ma&icirc;tris&eacute; en termes de co&ucirc;ts. Cependant, cela s&#39;accompagne d&#39;un compromis inh&eacute;rent de vitesses d&#39;inf&eacute;rence plus lentes, n&eacute;cessitant une s&eacute;lection rigoureuse des LLM et une gestion vigilante de la RAM. Cette strat&eacute;gie s&#39;aligne sur la demande croissante de solutions d&#39;IA soucieuses de la confidentialit&eacute; et &eacute;conomes en ressources.</span></p><p class="c4"><span class="c0">Par d&eacute;faut, Ollama charge un mod&egrave;le sp&eacute;cifi&eacute; en m&eacute;moire sur demande. Cependant, si les requ&ecirc;tes API ult&eacute;rieures (par exemple, provenant de diff&eacute;rentes applications ou avec des param&egrave;tres variables comme context_size) introduisent des incompatibilit&eacute;s de configuration, Ollama peut d&eacute;charger puis recharger le mod&egrave;le. Cette permutation fr&eacute;quente augmente consid&eacute;rablement la latence et l&#39;utilisation des ressources.</span><span class="c14 c0">36</span><span class="c7 c0">&nbsp;Pour assurer un chargement persistant du mod&egrave;le et minimiser ces retards, le param&egrave;tre</span></p><p class="c4"><span class="c0">duration: -1 peut &ecirc;tre d&eacute;fini lors du chargement d&#39;un mod&egrave;le via le point de terminaison /api/load d&#39;Ollama. Cette configuration indique &agrave; Ollama de maintenir le mod&egrave;le charg&eacute; en m&eacute;moire ind&eacute;finiment, quelle que soit l&#39;activit&eacute; ult&eacute;rieure ou les requ&ecirc;tes de point de terminaison.</span><span class="c14 c0">36</span><span class="c7 c0">&nbsp;De plus, la standardisation de la</span></p><p class="c4"><span class="c0">context_size sur toutes les applications ou processus interagissant avec la m&ecirc;me instance Ollama est cruciale. Des param&egrave;tres de contexte coh&eacute;rents emp&ecirc;chent le rechargement du mod&egrave;le pour s&#39;adapter &agrave; diff&eacute;rentes configurations, contribuant ainsi &agrave; des performances stables et efficaces.</span><span class="c13 c0">36</span></p><p class="c4 c10"><span class="c13 c0"></span></p><h2 class="c17"><span class="c9 c26">3. Architecture D&eacute;taill&eacute;e et Mise en &OElig;uvre avec Docker Compose</span></h2><p class="c4 c10"><span class="c7 c21"></span></p><p class="c4"><span class="c7 c0">La mise en place de ce syst&egrave;me de recherche neurale et RAG auto-h&eacute;berg&eacute; repose sur une architecture modulaire et une orchestration efficace via Docker Compose.</span></p><p class="c4 c10"><span class="c7 c0"></span></p><h3 class="c22"><span class="c6">3.1. Structure de l&#39;Application Vespa</span></h3><p class="c4 c10"><span class="c7 c21"></span></p><p class="c4"><span class="c7 c0">Vespa est le c&oelig;ur de la recherche et du classement. Pour int&eacute;grer ColBERT, une configuration sp&eacute;cifique est n&eacute;cessaire.</span></p><ul class="c1 lst-kix_list_3-0 start"><li class="c4 c12 li-bullet-0"><span class="c8">Sch&eacute;ma de Document pour les Plongements ColBERT :</span><span class="c0">&nbsp;Un sch&eacute;ma de document Vespa doit &ecirc;tre d&eacute;fini pour stocker les plongements multivectoriels de ColBERT. Chaque document dans Paperless-ngx (ou ses fragments) sera repr&eacute;sent&eacute; dans Vespa par un champ tenseur multivectoriel. Par exemple, un champ colbert_embeddings de type tensor&lt;float&gt;(token{}, x{}) peut &ecirc;tre utilis&eacute;, o&ugrave; token{} repr&eacute;sente les diff&eacute;rents jetons et x{} la dimension du plongement de chaque jeton (par exemple, 64 ou 128). L&#39;indexation HNSW sera activ&eacute;e sur ce champ pour permettre une recherche ANN efficace.</span><span class="c13 c0">7</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Profils de Classement pour l&#39;Interaction Tardive :</span><span class="c0">&nbsp;Pour impl&eacute;menter le m&eacute;canisme d&#39;interaction tardive de ColBERT (MaxSim), des profils de classement personnalis&eacute;s doivent &ecirc;tre d&eacute;finis dans Vespa. Cela implique l&#39;utilisation de fonctions tensorielles pour calculer la similarit&eacute; entre les plongements de jetons de la requ&ecirc;te et du document. La fonction closeness() de Vespa peut &ecirc;tre utilis&eacute;e pour trouver le vecteur le plus proche dans un champ multivectoriel. Des expressions de classement complexes peuvent &ecirc;tre construites pour agr&eacute;ger ces similarit&eacute;s au niveau des jetons, refl&eacute;tant le score MaxSim de ColBERT. L&#39;utilisation de phases de classement (par exemple, first-phase pour une r&eacute;cup&eacute;ration rapide et second-phase pour un reranking pr&eacute;cis avec ColBERT) est essentielle pour optimiser les performances.</span><span class="c13 c0">7</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Int&eacute;gration de l&#39;Embedder ColBERT :</span><span class="c0">&nbsp;Vespa peut int&eacute;grer des embedders directement. Il est possible d&#39;utiliser l&#39;embedder natif de Vespa pour ColBERT ou d&#39;int&eacute;grer un mod&egrave;le ColBERT pr&eacute;-entra&icirc;n&eacute; (par exemple, colbert-ir/colbertv2.0 via ONNX) pour g&eacute;n&eacute;rer les plongements de jetons au moment de l&#39;ingestion des documents et des requ&ecirc;tes.</span><span class="c13 c0">8</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Gestion des Donn&eacute;es Multimodales :</span><span class="c0">&nbsp;Bien que l&#39;accent soit mis sur le texte, Vespa prend en charge l&#39;indexation multimodale. Si Paperless-ngx AI venait &agrave; extraire des informations visuelles (par exemple, via ColiPali), Vespa pourrait indexer ces plongements d&#39;images &agrave; c&ocirc;t&eacute; des plongements de texte, permettant une recherche encore plus riche.</span><span class="c0 c13">16</span></li></ul><p class="c4 c10"><span class="c13 c0"></span></p><h3 class="c22"><span class="c6">3.2. Configuration de Paperless-ngx AI pour l&#39;Extraction et l&#39;Exportation</span></h3><p class="c4 c10"><span class="c7 c21"></span></p><p class="c4"><span class="c7 c0">Paperless-ngx AI g&eacute;rera l&#39;ingestion des documents et l&#39;extraction initiale des m&eacute;tadonn&eacute;es, agissant comme la source de donn&eacute;es pour Vespa.</span></p><ul class="c1 lst-kix_list_4-0 start"><li class="c4 c12 li-bullet-0"><span class="c8">Ingestion et OCR :</span><span class="c0">&nbsp;Paperless-ngx surveille un dossier de consommation ou re&ccedil;oit des documents via son interface web ou API. Il effectue l&#39;OCR sur les documents sans texte int&eacute;gr&eacute;, convertissant les images en texte consultable. Les documents sont stock&eacute;s au format PDF/A.</span><span class="c13 c0">6</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Extraction Automatis&eacute;e de M&eacute;tadonn&eacute;es :</span><span class="c0">&nbsp;Paperless-ngx utilise l&#39;apprentissage automatique pour attribuer automatiquement des tags, des correspondants et des types de documents. Ces m&eacute;tadonn&eacute;es structur&eacute;es sont cruciales pour le filtrage dans Vespa.</span><span class="c13 c0">5</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Acc&egrave;s aux Donn&eacute;es pour l&#39;Indexation Vespa :</span><span class="c7 c0">&nbsp;Pour que Vespa puisse indexer les documents et leurs m&eacute;tadonn&eacute;es, plusieurs approches sont possibles :</span></li></ul><ul class="c1 lst-kix_list_5-1 start"><li class="c4 c18 li-bullet-0"><span class="c8">API Paperless-ngx :</span><span class="c0">&nbsp;L&#39;API REST de Paperless-ngx permet de r&eacute;cup&eacute;rer le contenu textuel des documents et leurs m&eacute;tadonn&eacute;es (tags, correspondants, etc.).</span><span class="c14 c0">27</span><span class="c7 c0">&nbsp;Un script externe pourrait interroger cette API, g&eacute;n&eacute;rer les plongements ColBERT et les envoyer &agrave; Vespa.</span></li><li class="c4 c18 li-bullet-0"><span class="c8">Exportateur de Documents :</span><span class="c0">&nbsp;L&#39;outil document_exporter de Paperless-ngx peut exporter tous les documents, les vignettes, les m&eacute;tadonn&eacute;es et le contenu de la base de donn&eacute;es vers un dossier sp&eacute;cifi&eacute;. Cela est id&eacute;al pour les chargements initiaux en masse ou les mises &agrave; jour par lots, permettant &agrave; un processus externe de lire ces donn&eacute;es, de g&eacute;n&eacute;rer des plongements et de les alimenter &agrave; Vespa.</span><span class="c13 c0">29</span></li><li class="c4 c18 li-bullet-0"><span class="c8">Acc&egrave;s Direct aux Volumes :</span><span class="c0">&nbsp;Si les conteneurs Docker sont configur&eacute;s avec des volumes partag&eacute;s, un service d&eacute;di&eacute; pourrait acc&eacute;der directement aux fichiers de documents et &agrave; la base de donn&eacute;es de Paperless-ngx (par exemple, SQLite ou PostgreSQL) pour extraire les informations n&eacute;cessaires. Cependant, cela n&eacute;cessite une gestion attentive des d&eacute;pendances et de la coh&eacute;rence des donn&eacute;es.</span><span class="c13 c0">29</span></li></ul><p class="c4 c10"><span class="c13 c0"></span></p><h3 class="c22"><span class="c6">3.3. Configuration d&#39;Ollama pour l&#39;Inf&eacute;rence LLM sur CPU</span></h3><p class="c4 c10"><span class="c7 c21"></span></p><p class="c4"><span class="c7 c0">Ollama sera le moteur du LLM, fournissant des capacit&eacute;s de g&eacute;n&eacute;ration de texte bas&eacute;es sur le contexte r&eacute;cup&eacute;r&eacute;.</span></p><ul class="c1 lst-kix_list_6-0 start"><li class="c4 c12 li-bullet-0"><span class="c8">S&eacute;lection du Mod&egrave;le LLM sans GPU :</span><span class="c0">&nbsp;Pour une inf&eacute;rence sur CPU uniquement, il est imp&eacute;ratif de choisir des mod&egrave;les LLM petits et fortement quantifi&eacute;s. Les mod&egrave;les comme Mistral 7B (quantifi&eacute;), Llama 2 (7B ou 13B, quantifi&eacute;), Gemma 7B (quantifi&eacute;) ou Phi-3 Mini sont des candidats appropri&eacute;s. La quantification (par exemple, 4 bits) est essentielle pour r&eacute;duire l&#39;empreinte m&eacute;moire et am&eacute;liorer la vitesse d&#39;inf&eacute;rence sur CPU.</span><span class="c13 c0">30</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Persistance du Mod&egrave;le :</span><span class="c0">&nbsp;Pour &eacute;viter les rechargements fr&eacute;quents du mod&egrave;le qui augmentent la latence, il est recommand&eacute; de configurer Ollama pour maintenir le mod&egrave;le charg&eacute; en m&eacute;moire en utilisant le param&egrave;tre duration: -1 via le point de terminaison /api/load. De plus, la standardisation de la context_size sur toutes les applications interagissant avec Ollama est cruciale.</span><span class="c13 c0">36</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Exposition de l&#39;API Ollama :</span><span class="c0">&nbsp;Ollama expose une API (g&eacute;n&eacute;ralement sur le port 11434) qui sera utilis&eacute;e par l&#39;application RAG pour envoyer des requ&ecirc;tes et recevoir des r&eacute;ponses du LLM.</span><span class="c13 c0">13</span></li></ul><p class="c4 c10"><span class="c13 c0"></span></p><h3 class="c22"><span class="c6">3.4. Orchestration avec Docker Compose</span></h3><p class="c4 c10"><span class="c7 c21"></span></p><p class="c4"><span class="c7 c0">Docker Compose simplifiera le d&eacute;ploiement et la gestion des services interconnect&eacute;s.</span></p><ul class="c1 lst-kix_list_7-0 start"><li class="c4 c12 li-bullet-0"><span class="c8">R&eacute;seau Docker Partag&eacute; :</span><span class="c0">&nbsp;Un r&eacute;seau Docker Compose unique sera cr&eacute;&eacute; par d&eacute;faut, permettant &agrave; tous les services (Vespa, Paperless-ngx, Ollama et tout service interm&eacute;diaire) de communiquer entre eux en utilisant leurs noms de service comme noms d&#39;h&ocirc;te.</span><span class="c13 c0">39</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Structure du Fichier docker-compose.yml :</span><span class="c7 c0">&nbsp;Le fichier docker-compose.yml d&eacute;finira les services suivants :</span></li></ul><ul class="c1 lst-kix_list_8-1 start"><li class="c4 c18 li-bullet-0"><span class="c8">vespa :</span><span class="c0">&nbsp;Utilise l&#39;image vespaengine/vespa. Les volumes persistants seront configur&eacute;s pour les donn&eacute;es de Vespa. Les ports n&eacute;cessaires (par exemple, 8080 pour les requ&ecirc;tes/alimentation, 19071 pour le d&eacute;ploiement) seront mapp&eacute;s vers l&#39;h&ocirc;te.</span><span class="c13 c0">41</span></li><li class="c4 c18 li-bullet-0"><span class="c8">paperless-ngx :</span><span class="c0">&nbsp;Utilise l&#39;image Paperless-ngx. Des volumes seront mont&eacute;s pour les documents (media), les donn&eacute;es (data) et la base de donn&eacute;es (pgdata ou dbdata).</span><span class="c14 c0">29</span><span class="c0">&nbsp;Les variables d&#39;environnement pour la configuration de Paperless-ngx seront d&eacute;finies.</span><span class="c13 c0">37</span></li><li class="c4 c18 li-bullet-0"><span class="c8">ollama :</span><span class="c0">&nbsp;Utilise l&#39;image ollama/ollama. Un volume persistant sera configur&eacute; pour stocker les mod&egrave;les LLM t&eacute;l&eacute;charg&eacute;s (/root/.ollama). Le port 11434 sera mapp&eacute; vers l&#39;h&ocirc;te pour l&#39;acc&egrave;s &agrave; l&#39;API.</span><span class="c13 c0">13</span></li><li class="c4 c18 li-bullet-0"><span class="c8">Service Interm&eacute;diaire (Python/Node.js) :</span><span class="c7 c0">&nbsp;Un service personnalis&eacute; sera n&eacute;cessaire pour orchestrer le flux RAG :</span></li></ul><ul class="c1 lst-kix_list_9-2 start"><li class="c4 c16 li-bullet-0"><span class="c7 c0">R&eacute;cup&eacute;rer le contenu et les m&eacute;tadonn&eacute;es de Paperless-ngx (via API ou export).</span></li><li class="c4 c16 li-bullet-0"><span class="c7 c0">G&eacute;n&eacute;rer les plongements ColBERT pour les documents et les envoyer &agrave; Vespa pour l&#39;indexation.</span></li><li class="c4 c16 li-bullet-0"><span class="c7 c0">Recevoir les requ&ecirc;tes de l&#39;utilisateur.</span></li><li class="c4 c16 li-bullet-0"><span class="c7 c0">G&eacute;n&eacute;rer les plongements ColBERT pour la requ&ecirc;te de l&#39;utilisateur.</span></li><li class="c4 c16 li-bullet-0"><span class="c7 c0">Interroger Vespa pour la r&eacute;cup&eacute;ration de documents pertinents (en utilisant les m&eacute;tadonn&eacute;es de Paperless-ngx comme filtres et ColBERT pour le classement s&eacute;mantique).</span></li><li class="c4 c16 li-bullet-0"><span class="c7 c0">Envoyer le contexte r&eacute;cup&eacute;r&eacute; et la requ&ecirc;te de l&#39;utilisateur &agrave; Ollama pour la g&eacute;n&eacute;ration de r&eacute;ponses.</span></li><li class="c4 c16 li-bullet-0"><span class="c7 c0">Retourner la r&eacute;ponse du LLM &agrave; l&#39;utilisateur.</span></li></ul><ul class="c1 lst-kix_list_7-0"><li class="c4 c12 li-bullet-0"><span class="c8">Volumes Persistants :</span><span class="c0">&nbsp;L&#39;utilisation de volumes nomm&eacute;s Docker est cruciale pour garantir la persistance des donn&eacute;es (documents Paperless-ngx, index Vespa, mod&egrave;les Ollama) m&ecirc;me si les conteneurs sont arr&ecirc;t&eacute;s ou recr&eacute;&eacute;s.</span><span class="c13 c0">13</span></li></ul><p class="c4 c10"><span class="c13 c0"></span></p><h3 class="c22"><span class="c6">3.5. Flux de Donn&eacute;es et d&#39;Interaction</span></h3><p class="c4 c10"><span class="c7 c21"></span></p><ol class="c1 lst-kix_list_a-0 start" start="1"><li class="c4 c12 li-bullet-0"><span class="c8">Ingestion de Documents :</span><span class="c0">&nbsp;Les documents sont ajout&eacute;s &agrave; Paperless-ngx (via num&eacute;risation, e-mail, glisser-d&eacute;poser).</span><span class="c14 c0">23</span><span class="c0">&nbsp;Paperless-ngx effectue l&#39;OCR et l&#39;extraction initiale des m&eacute;tadonn&eacute;es.</span><span class="c13 c0">6</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Indexation des Documents (Vespa) :</span><span class="c7 c0">&nbsp;Un service interm&eacute;diaire (par exemple, un script Python) surveille les nouveaux documents dans Paperless-ngx (via API ou exportateur). Pour chaque document, il :</span></li></ol><ul class="c1 lst-kix_list_b-1 start"><li class="c4 c18 li-bullet-0"><span class="c7 c0">Extrait le texte et les m&eacute;tadonn&eacute;es.</span></li><li class="c4 c18 li-bullet-0"><span class="c0 c7">G&eacute;n&egrave;re les plongements multivectoriels ColBERT pour le contenu du document.</span></li><li class="c4 c18 li-bullet-0"><span class="c0">Envoie le document (texte, m&eacute;tadonn&eacute;es, plongements ColBERT) &agrave; Vespa pour l&#39;indexation. Vespa stocke les plongements et construit son index HNSW.</span><span class="c13 c0">7</span></li></ul><ol class="c1 lst-kix_list_a-0" start="3"><li class="c4 c12 li-bullet-0"><span class="c8">Requ&ecirc;te Utilisateur :</span><span class="c7 c0">&nbsp;L&#39;utilisateur soumet une requ&ecirc;te au service interm&eacute;diaire.</span></li><li class="c4 c12 li-bullet-0"><span class="c8">R&eacute;cup&eacute;ration Hybride (Vespa) :</span><span class="c7 c0">&nbsp;Le service interm&eacute;diaire :</span></li></ol><ul class="c1 lst-kix_list_c-1 start"><li class="c4 c18 li-bullet-0"><span class="c7 c0">G&eacute;n&egrave;re les plongements multivectoriels ColBERT pour la requ&ecirc;te de l&#39;utilisateur.</span></li><li class="c4 c18 li-bullet-0"><span class="c7 c0">Construit une requ&ecirc;te Vespa qui combine les filtres bas&eacute;s sur les m&eacute;tadonn&eacute;es de Paperless-ngx (par exemple, type de document, correspondant) avec la recherche de voisins les plus proches sur les plongements ColBERT.</span></li><li class="c4 c18 li-bullet-0"><span class="c0">Vespa ex&eacute;cute cette requ&ecirc;te, utilisant son classement par phases pour r&eacute;cup&eacute;rer un ensemble de documents pertinents, avec ColBERT assurant un classement fin.</span><span class="c13 c0">7</span></li></ul><ol class="c1 lst-kix_list_a-0" start="5"><li class="c4 c12 li-bullet-0"><span class="c8">G&eacute;n&eacute;ration Augment&eacute;e (Ollama) :</span><span class="c0">&nbsp;Les extraits de documents les plus pertinents r&eacute;cup&eacute;r&eacute;s par Vespa sont envoy&eacute;s, avec la requ&ecirc;te originale de l&#39;utilisateur, &agrave; l&#39;API Ollama. Le LLM h&eacute;berg&eacute; par Ollama g&eacute;n&egrave;re une r&eacute;ponse bas&eacute;e sur ce contexte fourni, garantissant une r&eacute;ponse fond&eacute;e et pertinente.</span><span class="c13 c0">3</span></li><li class="c4 c12 li-bullet-0"><span class="c8">R&eacute;ponse &agrave; l&#39;Utilisateur :</span><span class="c7 c0">&nbsp;Le service interm&eacute;diaire renvoie la r&eacute;ponse g&eacute;n&eacute;r&eacute;e par le LLM &agrave; l&#39;utilisateur.</span></li></ol><p class="c4 c10"><span class="c7 c0"></span></p><h2 class="c17"><span class="c26 c9">4. Conclusions et Recommandations</span></h2><p class="c4 c10"><span class="c7 c21"></span></p><p class="c4"><span class="c7 c0">La mise en &oelig;uvre d&#39;un syst&egrave;me de recherche neurale de nouvelle g&eacute;n&eacute;ration avec ColBERT et Vespa, int&eacute;gr&eacute; &agrave; Paperless-ngx AI et Ollama sur une architecture sans GPU, est une entreprise ambitieuse mais r&eacute;alisable qui offre des avantages significatifs. Le syst&egrave;me propos&eacute; transforme la gestion de documents en une exp&eacute;rience de connaissance interactive et intelligente.</span></p><p class="c4"><span class="c7 c0">La synergie entre la gestion structur&eacute;e des documents de Paperless-ngx et la compr&eacute;hension s&eacute;mantique fine de ColBERT, orchestr&eacute;e par les capacit&eacute;s de recherche et de classement de Vespa, permet une r&eacute;cup&eacute;ration d&#39;informations d&#39;une pr&eacute;cision in&eacute;gal&eacute;e. Les m&eacute;tadonn&eacute;es de Paperless-ngx agissent comme des filtres puissants pour affiner rapidement l&#39;ensemble des documents pertinents, tandis que ColBERT excelle dans le reclassement de cet ensemble avec une compr&eacute;hension contextuelle profonde. Cette approche hybride combine le meilleur de la recherche bas&eacute;e sur les attributs et de la recherche s&eacute;mantique.</span></p><p class="c4"><span class="c7 c0">L&#39;int&eacute;gration d&#39;Ollama sur un environnement CPU uniquement, bien que pr&eacute;sentant des compromis en termes de vitesse d&#39;inf&eacute;rence, est rendue viable par la s&eacute;lection rigoureuse de mod&egrave;les LLM petits et fortement quantifi&eacute;s. Cette strat&eacute;gie permet de maintenir un syst&egrave;me enti&egrave;rement auto-h&eacute;berg&eacute;, garantissant une confidentialit&eacute; totale des donn&eacute;es et &eacute;liminant les co&ucirc;ts d&#39;API r&eacute;currents. Docker Compose simplifie consid&eacute;rablement le d&eacute;ploiement et la gestion de cette architecture complexe, en assurant une communication fluide entre les services et une persistance des donn&eacute;es.</span></p><p class="c4"><span class="c3">Recommandations pour la Mise en &OElig;uvre :</span></p><ol class="c1 lst-kix_list_d-0 start" start="1"><li class="c4 c12 li-bullet-0"><span class="c8">Prioriser la Quantification des LLM :</span><span class="c7 c0">&nbsp;&Eacute;tant donn&eacute; la contrainte &quot;sans GPU&quot;, il est imp&eacute;ratif de se concentrer sur l&#39;utilisation de mod&egrave;les LLM fortement quantifi&eacute;s (par exemple, 4 bits) avec Ollama. Des tests de performance rigoureux avec diff&eacute;rents mod&egrave;les (Mistral 7B, Llama 2 7B/13B, Gemma 7B, Phi-3 Mini) sont recommand&eacute;s pour trouver le meilleur &eacute;quilibre entre qualit&eacute; de r&eacute;ponse et vitesse d&#39;inf&eacute;rence sur le mat&eacute;riel CPU disponible.</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Optimiser l&#39;Indexation ColBERT dans Vespa :</span><span class="c7 c0">&nbsp;Tirer pleinement parti du support multivectoriel natif de Vespa pour les plongements ColBERT. Configurer les profils de classement de Vespa pour ex&eacute;cuter efficacement le calcul MaxSim, en utilisant des phases de classement pour optimiser les performances de r&eacute;cup&eacute;ration et de re-classement.</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Mettre en Place un Service d&#39;Orchestration Robuste :</span><span class="c7 c0">&nbsp;D&eacute;velopper un service interm&eacute;diaire (par exemple, en Python) qui g&egrave;re l&#39;extraction des donn&eacute;es de Paperless-ngx, la g&eacute;n&eacute;ration des plongements ColBERT, l&#39;alimentation de Vespa, l&#39;interrogation de Vespa et l&#39;interaction avec Ollama. Ce service doit &ecirc;tre con&ccedil;u pour g&eacute;rer les erreurs et assurer la coh&eacute;rence des donn&eacute;es.</span></li><li class="c4 c12 li-bullet-0"><span class="c8">G&eacute;rer la Persistance des Mod&egrave;les Ollama :</span><span class="c7 c0">&nbsp;Configurer Ollama pour maintenir les mod&egrave;les LLM charg&eacute;s en m&eacute;moire en utilisant le param&egrave;tre duration: -1 et en standardisant la context_size pour toutes les interactions. Cela minimisera les d&eacute;lais de rechargement et am&eacute;liorera la r&eacute;activit&eacute; du syst&egrave;me.</span></li><li class="c4 c12 li-bullet-0"><span class="c8">Surveiller les Ressources Syst&egrave;me :</span><span class="c7 c0">&nbsp;Une surveillance attentive de l&#39;utilisation de la RAM et du CPU sera cruciale, en particulier pour le service Ollama. Des ajustements dans la taille du mod&egrave;le LLM ou la configuration de la quantification pourraient &ecirc;tre n&eacute;cessaires pour maintenir des performances acceptables.</span></li><li class="c4 c12 li-bullet-0"><span class="c8">S&eacute;curit&eacute; et Acc&egrave;s :</span><span class="c7 c0">&nbsp;Assurer la s&eacute;curit&eacute; de l&#39;API Paperless-ngx et de l&#39;instance Ollama, surtout si le syst&egrave;me est expos&eacute; au r&eacute;seau. Utiliser des cl&eacute;s API robustes et envisager un reverse proxy pour une couche de s&eacute;curit&eacute; suppl&eacute;mentaire.</span></li><li class="c4 c12 li-bullet-0"><span class="c8">It&eacute;rer et &Eacute;valuer :</span><span class="c7 c0">&nbsp;Commencer par une impl&eacute;mentation minimale viable, puis it&eacute;rer en fonction des besoins. &Eacute;valuer r&eacute;guli&egrave;rement la pertinence des r&eacute;sultats de recherche et la qualit&eacute; des r&eacute;ponses du LLM pour affiner les configurations et les mod&egrave;les.</span></li></ol><p class="c23"><span class="c7 c0">En suivant ces recommandations, il est possible de construire un syst&egrave;me de gestion de documents intelligent et priv&eacute;, capable de fournir des r&eacute;ponses pr&eacute;cises et contextuelles, m&ecirc;me dans un environnement mat&eacute;riel contraint.</span></p><h4 class="c32"><span class="c9 c29">Sources des citations</span></h4><ol class="c1 lst-kix_list_e-0 start" start="1"><li class="c19 c12 li-bullet-0"><span class="c15">Neural search: Definition, how it works, benefits and more - Meilisearch, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://www.meilisearch.com/blog/neural-search&amp;sa=D&amp;source=editors&amp;ust=1753093180565820&amp;usg=AOvVaw1yOD42h9DQMmxft9GV_tEo">https://www.meilisearch.com/blog/neural-search</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">What is Neural Search and How Does It Work? - Moveworks, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://www.moveworks.com/us/en/resources/blog/what-is-neural-search&amp;sa=D&amp;source=editors&amp;ust=1753093180566157&amp;usg=AOvVaw0E2C9JFiI9A41gnWX2uyCD">https://www.moveworks.com/us/en/resources/blog/what-is-neural-search</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">What is Retrieval Augmented Generation (RAG) for LLMs? - Hopsworks, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://www.hopsworks.ai/dictionary/retrieval-augmented-generation-llm&amp;sa=D&amp;source=editors&amp;ust=1753093180566505&amp;usg=AOvVaw3fMdRyJia1RNvTGV0n7Bj-">https://www.hopsworks.ai/dictionary/retrieval-augmented-generation-llm</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">What is RAG? - Retrieval-Augmented Generation AI Explained - AWS, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://aws.amazon.com/what-is/retrieval-augmented-generation/&amp;sa=D&amp;source=editors&amp;ust=1753093180566800&amp;usg=AOvVaw06fkTztz3ye2snOODBxa9U">https://aws.amazon.com/what-is/retrieval-augmented-generation/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Paperless-AI on Synology NAS: AI-Powered Document Management with Paperless-NGX, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://blog.admin-intelligence.de/en/paperless-ai-on-synology-nas-ai-powered-document-management-with-paperless-ngx/&amp;sa=D&amp;source=editors&amp;ust=1753093180567220&amp;usg=AOvVaw2ljhOoTwSL_f5Rrki3pzD_">https://blog.admin-intelligence.de/en/paperless-ai-on-synology-nas-ai-powered-document-management-with-paperless-ngx/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Paperless-ngx, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://docs.paperless-ngx.com/&amp;sa=D&amp;source=editors&amp;ust=1753093180567404&amp;usg=AOvVaw1e3EDYhgimcrr-HztgHik-">https://docs.paperless-ngx.com/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Features - Vespa Documentation, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://docs.vespa.ai/en/features.html&amp;sa=D&amp;source=editors&amp;ust=1753093180567604&amp;usg=AOvVaw3jIKKu5PGHzI0Si5aBcyau">https://docs.vespa.ai/en/features.html</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Vespa.ai - Vespa.ai, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://vespa.ai/&amp;sa=D&amp;source=editors&amp;ust=1753093180567810&amp;usg=AOvVaw3cukSPVhdTdNZsimpUgyhR">https://vespa.ai/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction ..., consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=http://arxiv.org/pdf/2408.16672&amp;sa=D&amp;source=editors&amp;ust=1753093180568096&amp;usg=AOvVaw0PkZXXQMiE8X7wf0YSygzz">http://arxiv.org/pdf/2408.16672</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Working with ColBERT - Qdrant, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://qdrant.tech/documentation/fastembed/fastembed-colbert/&amp;sa=D&amp;source=editors&amp;ust=1753093180568337&amp;usg=AOvVaw1kodiopumzF-sIobnsM-OO">https://qdrant.tech/documentation/fastembed/fastembed-colbert/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Accelerating LLM Inference with OLAMA: A GPU-Enabled Approach - GoPenAI, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://blog.gopenai.com/accelerating-llm-inference-with-olama-a-gpu-enabled-approach-bc1cc27dd246&amp;sa=D&amp;source=editors&amp;ust=1753093180568699&amp;usg=AOvVaw1YOTk7aI_YqvkgeH7bMDT9">https://blog.gopenai.com/accelerating-llm-inference-with-olama-a-gpu-enabled-approach-bc1cc27dd246</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Using Ollama to host an LLM on CPU-only equipment to enable a local chatbot and LLM API, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://blog.gordonbuchan.com/blog/index.php/2025/01/11/using-ollama-to-host-an-llm-on-cpu-only-equipment-to-enable-a-local-chatbot-and-llm-api-server/&amp;sa=D&amp;source=editors&amp;ust=1753093180569160&amp;usg=AOvVaw12Uey6sNMvDj7NXWvneBJg">https://blog.gordonbuchan.com/blog/index.php/2025/01/11/using-ollama-to-host-an-llm-on-cpu-only-equipment-to-enable-a-local-chatbot-and-llm-api-server/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Setting Up Ollama with Open-WebUI: A Docker Compose Guide - Archy.net, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://www.archy.net/setting-up-ollama-with-open-webui-a-docker-compose-guide/&amp;sa=D&amp;source=editors&amp;ust=1753093180569462&amp;usg=AOvVaw0LtgdOT0jsWas-jx2KsAOE">https://www.archy.net/setting-up-ollama-with-open-webui-a-docker-compose-guide/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">How to use Ollama and Open WebUI with Docker Compose [Part 4] - Geshan Manandhar, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://geshan.com.np/blog/2025/02/ollama-docker-compose/&amp;sa=D&amp;source=editors&amp;ust=1753093180569784&amp;usg=AOvVaw0eFK2tdmO1wUntgheLP20I">https://geshan.com.np/blog/2025/02/ollama-docker-compose/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">What is ColBERT and how does it differ from standard bi-encoder approaches? - Milvus, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://milvus.io/ai-quick-reference/what-is-colbert-and-how-does-it-differ-from-standard-biencoder-approaches&amp;sa=D&amp;source=editors&amp;ust=1753093180570161&amp;usg=AOvVaw1_L8XV7REQZS__InV0r1aH">https://milvus.io/ai-quick-reference/what-is-colbert-and-how-does-it-differ-from-standard-biencoder-approaches</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">ColBERT + PaliGemma = ColiPali for Document Retrieval | by Samvardhan V G | Medium, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://medium.com/@samvardhan777/colbert-paligemma-colipali-for-document-retrieval-3bb8cc80326c&amp;sa=D&amp;source=editors&amp;ust=1753093180570589&amp;usg=AOvVaw1EpNH5eLfcQH0CCdus-3lL">https://medium.com/@samvardhan777/colbert-paligemma-colipali-for-document-retrieval-3bb8cc80326c</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">How the ColBERT re-ranker model in a RAG system works - IBM Developer, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://developer.ibm.com/articles/how-colbert-works/&amp;sa=D&amp;source=editors&amp;ust=1753093180570915&amp;usg=AOvVaw1RUPgmokfm3ij6bKQWOXNb">https://developer.ibm.com/articles/how-colbert-works/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever - arXiv, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/html/2408.16672v4&amp;sa=D&amp;source=editors&amp;ust=1753093180571200&amp;usg=AOvVaw1w4hDwHH3LSheZowHwE48I">https://arxiv.org/html/2408.16672v4</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">RAGatouille | &#129436;&#65039; LangChain, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://python.langchain.com/docs/integrations/providers/ragatouille/&amp;sa=D&amp;source=editors&amp;ust=1753093180571485&amp;usg=AOvVaw1E6dDbijr4i7Y-9WtYFzw_">https://python.langchain.com/docs/integrations/providers/ragatouille/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Revolutionizing Semantic Search with Multi-Vector HNSW Indexing in Vespa, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://blog.vespa.ai/semantic-search-with-multi-vector-indexing/&amp;sa=D&amp;source=editors&amp;ust=1753093180571817&amp;usg=AOvVaw2jx8jX3yZkTPrySKqi7T8T">https://blog.vespa.ai/semantic-search-with-multi-vector-indexing/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Ranking - Vespa Documentation, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://docs.vespa.ai/en/ranking.html&amp;sa=D&amp;source=editors&amp;ust=1753093180572042&amp;usg=AOvVaw3BiddZHDtJ1h98tpUazI51">https://docs.vespa.ai/en/ranking.html</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">getting-started-ranking.md - vespa-engine/documentation - GitHub, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://github.com/vespa-engine/documentation/blob/master/en/getting-started-ranking.md&amp;sa=D&amp;source=editors&amp;ust=1753093180572396&amp;usg=AOvVaw3Iv0AEbtp2SUv6qj6kDrJM">https://github.com/vespa-engine/documentation/blob/master/en/getting-started-ranking.md</a></span></li><li class="c12 c19 li-bullet-0"><span class="c15">Paperless-ngx - Quickstart | Elest.io, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://elest.io/open-source/paperless-ngx/resources/quickstart&amp;sa=D&amp;source=editors&amp;ust=1753093180572656&amp;usg=AOvVaw2lURXgJqYPqvCU9OYgOkM0">https://elest.io/open-source/paperless-ngx/resources/quickstart</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Advanced Topics - Paperless-ngx, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://docs.paperless-ngx.com/advanced_usage/&amp;sa=D&amp;source=editors&amp;ust=1753093180572901&amp;usg=AOvVaw09LyFX1m60DHjNPERHZe8T">https://docs.paperless-ngx.com/advanced_usage/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Today&#39;s Deep-Dive: Paperless-ngx - Podcasts safeserver.de, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://podcast.safeserver.de/@deepdive/episodes/todays-deep-dive-paperless-ngx/transcript&amp;sa=D&amp;source=editors&amp;ust=1753093180573270&amp;usg=AOvVaw1A2vBa7pcd6M5L2qfO8bjw">https://podcast.safeserver.de/@deepdive/episodes/todays-deep-dive-paperless-ngx/transcript</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Usage Overview - Paperless-ngx, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://docs.paperless-ngx.com/usage/&amp;sa=D&amp;source=editors&amp;ust=1753093180573480&amp;usg=AOvVaw2-Sqg4G8vdcAEQI-5_YUhR">https://docs.paperless-ngx.com/usage/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">REST API - Paperless-ngx, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://docs.paperless-ngx.com/api/&amp;sa=D&amp;source=editors&amp;ust=1753093180573703&amp;usg=AOvVaw1rxh-tuk3_tq9gytoYwbkg">https://docs.paperless-ngx.com/api/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Paperless-ngx - Home Assistant, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://www.home-assistant.io/integrations/paperless_ngx/&amp;sa=D&amp;source=editors&amp;ust=1753093180573981&amp;usg=AOvVaw0Abo6ebRHj82fRUVbbolr9">https://www.home-assistant.io/integrations/paperless_ngx/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Administration - Paperless-ngx, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://docs.paperless-ngx.com/administration/&amp;sa=D&amp;source=editors&amp;ust=1753093180574200&amp;usg=AOvVaw1PWcqubEwhCpd1_Hx2sk6j">https://docs.paperless-ngx.com/administration/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Ollama: Run quantized LLMs on CPUs and GPUs &mdash; SkyPilot documentation, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://docs.skypilot.co/en/v0.9.2/examples/serving/ollama.html&amp;sa=D&amp;source=editors&amp;ust=1753093180574498&amp;usg=AOvVaw319iCdwuMg5Q8KFNaLK0Fx">https://docs.skypilot.co/en/v0.9.2/examples/serving/ollama.html</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Ollama CPU: Model Manager Script &amp; Inference with a Terminal UI - YouTube, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3Df_M4WfxJ4mQ&amp;sa=D&amp;source=editors&amp;ust=1753093180574816&amp;usg=AOvVaw0Q-NTZiuNESc1T_5A3HQZI">https://www.youtube.com/watch?v=f_M4WfxJ4mQ</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Running Small LLMs Locally: My Journey With and Without GPUs | by Ibrahim Sajid Malick, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://medium.com/@IbrahimMalick/running-small-llms-locally-my-journey-with-and-without-gpus-1e256cde33bb&amp;sa=D&amp;source=editors&amp;ust=1753093180575250&amp;usg=AOvVaw3GuyuOELhcTe-8rj7J4SIH">https://medium.com/@IbrahimMalick/running-small-llms-locally-my-journey-with-and-without-gpus-1e256cde33bb</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">seeking a fast local LLM solution using only CPU : r/ollama - Reddit, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://www.reddit.com/r/ollama/comments/1ir69xk/seeking_a_fast_local_llm_solution_using_only_cpu/&amp;sa=D&amp;source=editors&amp;ust=1753093180575634&amp;usg=AOvVaw07f-SVUBZ0Hu5r7BLJs6iL">https://www.reddit.com/r/ollama/comments/1ir69xk/seeking_a_fast_local_llm_solution_using_only_cpu/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Using Quantized Models with Ollama for Application Development - MachineLearningMastery.com, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://machinelearningmastery.com/using-quantized-models-with-ollama-for-application-development/&amp;sa=D&amp;source=editors&amp;ust=1753093180576064&amp;usg=AOvVaw0DPiDXD3el0HS3U2hrzoGN">https://machinelearningmastery.com/using-quantized-models-with-ollama-for-application-development/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">10 Best Small Local LLMs to Try Out (&lt; 8GB) - Apidog, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://apidog.com/blog/small-local-llm/&amp;sa=D&amp;source=editors&amp;ust=1753093180576302&amp;usg=AOvVaw2QJmS_p9miXTP6svSV6wrS">https://apidog.com/blog/small-local-llm/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Preventing Model Swapping In Ollama &mdash; A Guide To Persistent Loading - GoPenAI, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://blog.gopenai.com/preventing-model-swapping-in-ollama-a-guide-to-persistent-loading-f81f1dfb858d&amp;sa=D&amp;source=editors&amp;ust=1753093180576698&amp;usg=AOvVaw2-oATOmk1tkpkWoWncpwNB">https://blog.gopenai.com/preventing-model-swapping-in-ollama-a-guide-to-persistent-loading-f81f1dfb858d</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Configuration - Paperless-ngx, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://docs.paperless-ngx.com/configuration/&amp;sa=D&amp;source=editors&amp;ust=1753093180576959&amp;usg=AOvVaw1hrvHEpcljod7KTU5GlgtW">https://docs.paperless-ngx.com/configuration/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">How to Use Ollama (Complete Ollama Cheatsheet) - Apidog, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://apidog.com/blog/how-to-use-ollama/&amp;sa=D&amp;source=editors&amp;ust=1753093180577224&amp;usg=AOvVaw2_S4l5vwk_5iZJs05WG2lv">https://apidog.com/blog/how-to-use-ollama/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Networking With Docker Compose (Quick Guide) - Netmaker, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://www.netmaker.io/resources/docker-compose-network&amp;sa=D&amp;source=editors&amp;ust=1753093180577537&amp;usg=AOvVaw1RvhPGVkiMR6b4OjlcMCCv">https://www.netmaker.io/resources/docker-compose-network</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">Communication between multiple docker-compose projects, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://dev.to/iamrj846/communication-between-multiple-docker-compose-projects-223k&amp;sa=D&amp;source=editors&amp;ust=1753093180577867&amp;usg=AOvVaw3KSf0J-Udm7jtAPzlBvXo4">https://dev.to/iamrj846/communication-between-multiple-docker-compose-projects-223k</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">vespaengine/vespa - Docker Image, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://hub.docker.com/r/vespaengine/vespa/&amp;sa=D&amp;source=editors&amp;ust=1753093180578131&amp;usg=AOvVaw3yIFMUpZSF5aCHOcG_6Z_M">https://hub.docker.com/r/vespaengine/vespa/</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">docker-compose.yaml - vespa-engine/sample-apps - GitHub, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://github.com/vespa-engine/sample-apps/blob/master/examples/operations/multinode-HA/docker-compose.yaml&amp;sa=D&amp;source=editors&amp;ust=1753093180578510&amp;usg=AOvVaw3NjKeb_ZEAz66pP99DRl-R">https://github.com/vespa-engine/sample-apps/blob/master/examples/operations/multinode-HA/docker-compose.yaml</a></span></li><li class="c19 c12 li-bullet-0"><span class="c15">paperless-ngx-compose/docker-compose.env at main - GitHub, consult&eacute; le juillet 20, 2025, </span><span class="c5"><a class="c11" href="https://www.google.com/url?q=https://github.com/BWibo/paperless-ngx-compose/blob/main/docker-compose.env&amp;sa=D&amp;source=editors&amp;ust=1753093180578817&amp;usg=AOvVaw1Ko_NDc0dhudyCY9KDsmW2">https://github.com/BWibo/paperless-ngx-compose/blob/main/docker-compose.env</a></span></li></ol></body></html>